{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p><code>moduleprofiler</code> is a free open-source package to profile <code>torch.nn.Module</code> modules and obtain useful information to design a model that fits your needs and constraints at development time.</p> <p>With <code>moduleprofiler</code> you can:</p> <ul> <li>Calculate the number of parameters of your model.</li> <li>Trace the input and output sizes of each component of your model.</li> <li>Estimate the number of operations your model performs in a forward pass.</li> <li>Calculate per module and total inference time.</li> </ul> <p>All results can be obtained in one of the following formats:</p> <ul> <li><code>dict</code> (default output format)</li> <li><code>pandas.DataFrame</code> (to perform further calculations or filtering in your code)</li> <li><code>html</code> (to export as webpage)</li> <li><code>LaTeX</code> (to include in your publications)</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p><code>moduleprofiler</code> can be installed as any regular <code>python</code> module within your environment.</p> <p>Install from PyPI: <pre><code>python -m pip install moduleprofiler\n</code></pre></p> <p>Install from this repository: <pre><code>python -m pip install git+https://github.com/eagomez2/moduleprofiler.git\n</code></pre></p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Using <code>moduleprofiler</code> is simple. First, you need to create a <code>ModuleProfiler</code> object, and then you can run any method over your existing <code>torch.nn.Module</code> model.</p> basic_moduleprofiler_example.py<pre><code>import torch\nfrom moduleprofiler import ModuleProfiler\n\n# Input tensor\nx = torch.rand((1, 8))\n\n# Network\nnet = torch.nn.Linear(in_features=8, out_features=32)\n\n# Profiler\nprofiler = ModuleProfiler()\n\n# Calculate number of parameters in the model\nnet_params = profiler.count_params(module=net)\n\"\"\"\n{'__root__':\n  {\n      'type': 'Linear',\n      'trainable_params': 288,\n      'nontrainable_params': 0,\n      'trainable_params_dtype': torch.float32,\n      'trainable_params_size_bits': 9216,\n      'trainable_params_percent': 1.0,\n      'nontrainable_params_percent': 0.0\n  }\n}\n\"\"\"\n\n# Compute sizes of input and output tensors of each layer\nnet_io = profiler.trace_io_sizes(module=net, input=x)\n\"\"\"\n{'__root__':\n  {'type': 'Linear', 'input_size': (1, 8), 'output_size': (1, 32)}\n}\n\"\"\"\n\n# Estimate operations performed by each layer\nnet_ops = profiler.estimate_ops(module=net, input=x)\n\"\"\"\n{'__root__': {'type': 'Linear', 'ops': 512}}\n\"\"\"\n</code></pre> <p>Methods for generating the results in different formats such as <code>DataFrame</code>, <code>.csv</code> or <code>.html</code> have the same names, except for a suffixed ending. For example, <code>profiler.trace_io_sizes</code> becomes <code>profiler.trace_io_sizes_df</code> for a <code>DataFrame</code> output, or <code>profiler.trace_io_sizes_csv</code> to save results to a <code>.csv</code> file. Methods without a suffix will generate a <code>dict</code> output.</p> <ul> <li>For more in-depth tutorials please see the Tutorial section.</li> <li>For further information about specifics methods, please see the Documentation section.</li> <li>For further information about how the complexity of different layers is estimated, please see the Reference section.</li> </ul> <p>If you have any feedback or suggestions, feel free to open an issue in GitHub or reach out to the author. If this package contributed to your work, please consider citing it:</p> <pre><code>@misc{moduleprofiler,\n  author = {Esteban G\u00f3mez},\n  title  = {moduleprofiler},\n  year   = 2024,\n  url    = {https://github.com/eagomez2/moduleprofiler}\n}\n</code></pre>"},{"location":"#supported-modules","title":"Supported modules","text":"<p>By default, all methods support all modules as long as these are instances of <code>torch.nn.Module</code>. The only functionality where some modules might be missing is in the <code>estimate_ops</code> method that used to estimate the operations performed by a certain module. Modules that are not supported will result in <code>None</code> as returned value. However, it is possible to add both missing and custom modules as described in Tutorial.</p> Module Supported (ops) Version <code>torch.nn.Identity</code> 0.0.1 <code>torch.nn.Linear</code> 0.0.1 <code>torch.nn.Conv1d</code> 0.0.1 <code>torch.nn.Conv2d</code> 0.0.1 <code>torch.nn.ConvTranspose1d</code> 0.0.1 <code>torch.nn.ConvTranspose2d</code> 0.0.1 <code>torch.nn.GRUCell</code> 0.0.1 <code>torch.nn.GRU</code> 0.0.1 <code>torch.nn.LSTMCell</code> 0.0.1 <code>torch.nn.LSTM</code> 0.0.1 <code>torch.nn.MultiheadAttention</code> <code>torch.nn.ReLU</code> 0.0.1 <code>torch.nn.LeakyReLU</code> 0.0.1 <code>torch.nn.ELU</code> 0.0.1 <code>torch.nn.PReLU</code> 0.0.1 <code>torch.nn.Sigmoid</code> 0.0.1 <code>torch.nn.Softmax</code> 0.0.1 <code>torch.nn.Softplus</code> 0.0.1 <code>torch.nn.Tanh</code> 0.0.1 <code>torch.nn.AvgPool1d</code> 0.0.1 <code>torch.nn.AvgPool2d</code> 0.0.1 <code>torch.nn.AdaptiveMaxPool1d</code> 0.0.1 <code>torch.nn.AdaptiveMaxPool2d</code> 0.0.1 <code>torch.nn.MaxPool1d</code> 0.0.1 <code>torch.nn.MaxPool2d</code> 0.0.1 <code>torch.nn.LayerNorm</code> 0.0.1 <code>torch.nn.BatchNorm1d</code> 0.0.4 <code>torch.nn.BatchNorm2d</code> 0.0.4"},{"location":"documentation/","title":"Documentation","text":""},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler","title":"<code>ModuleProfiler</code>","text":"<p>Main class used to profile an arbitraty <code>nn.Module</code> and describe different specifications of it such as tracing input and output shapes, counting model parameters or estimating the number of operations the model peforms:</p> <p>Parameters:</p> Name Type Description Default <code>input_size_attr</code> <code>str</code> <p>Hidden attribute in the module under measurement used to store its input size.</p> <code>'__input_size__'</code> <code>output_size_attr</code> <code>str</code> <p>Hidden attribute in the module under measurement used to store its output size.</p> <code>'__output_size__'</code> <code>ops_attr</code> <code>str</code> <p>Hidden attribute in the module under measurement used to store the number of operations it performs.</p> <code>'__ops__'</code> <code>inference_start_attr</code> <code>str</code> <p>Hidden attribute used to store inference start times while timing a model.</p> <code>'__inference_start__'</code> <code>inference_end_attr</code> <code>str</code> <p>Hidden attribute used to store inference end times while timing a model.</p> <code>'__inference_end__'</code> <code>io_size_fn_map</code> <code>dict</code> <p>Dictionary containing a map between modules and  their corresponding functions useed to trace the its size.</p> <code>get_default_io_size_map()</code> <code>ops_fn_map</code> <code>dict</code> <p>Dictionary containing a map between modules and their corresponding function to estimate the number of operations.</p> <code>get_default_ops_map()</code> <code>exclude_from_ops</code> <code>Optional[List[Module]]</code> <p>Modules to exclude from ops estimations.</p> <code>None</code> <code>ts_fmt</code> <code>str</code> <p>Timestamp format used to print messages if <code>verbose=True</code>.</p> <code>'%Y-%m-%d %H:%M:%S'</code> <code>verbose</code> <code>bool</code> <p>If <code>True</code>, enabled verbose output mode.</p> <code>False</code>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.count_params","title":"<code>count_params(module, param_size=True, param_dtype=True, percent=True)</code>","text":"<p>Counts the number of parameters in a model.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>Model whose parameters will be counted.</p> required <code>param_size</code> <code>bool</code> <p>If <code>True</code>, the size in bits of each parameters will be calculated.</p> <code>True</code> <code>param_dtype</code> <code>bool</code> <p>If <code>True</code>, the data type of different parameters will be reported.</p> <code>True</code> <code>percent</code> <code>bool</code> <p>If <code>True</code>, the percentage each parameter represents with respect to the total amount of parameters of the model will be reported.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Analysis results containing the measured module names and each corresponding parameter count.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.count_params_csv","title":"<code>count_params_csv(file, *args, **kwargs)</code>","text":"<p>Same as <code>count_params</code> but saves a <code>.csv</code> file instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.count_params_df","title":"<code>count_params_df(*args, **kwargs)</code>","text":"<p>Same as <code>count_params</code> but returns a <code>DataFrame</code> instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.count_params_html","title":"<code>count_params_html(file, *args, **kwargs)</code>","text":"<p>Same as <code>count_params</code> but saves a <code>.html</code> file instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.count_params_latex","title":"<code>count_params_latex(*args, index=False, **kwargs)</code>","text":"<p>Same as <code>count_params</code> but returns a LaTeX output instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.estimate_inference_time","title":"<code>estimate_inference_time(module, input, eval=True, num_iters=1000, drop_first=100)</code>","text":"<p>Estimates the time spent on each module during the forward pass of a model. The final results are statistical aggregation of <code>num_iters</code> dropping the first <code>drop_first</code> iterations to avoid outliers caused by warmup routines.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>Input module.</p> required <code>input</code> <code>(Union[Tensor], Tuple[Tensor])</code> <p>Model input.</p> required <code>eval</code> <code>bool</code> <p>If <code>True</code>, the module is set to eval mode before computing the inference time.</p> <code>True</code> <code>num_iters</code> <code>int</code> <p>Number of iterations to be performed.</p> <code>1000</code> <code>drop_first</code> <code>int</code> <p>Inferences to be dropped before aggregating the results.</p> <code>100</code> <p>Returns:</p> Type Description <code>dict</code> <p>Measurement results.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.estimate_inference_time_csv","title":"<code>estimate_inference_time_csv(file, *args, **kwargs)</code>","text":"<p>Same as <code>estimate_inference_time</code> but saves a <code>.csv</code> file instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.estimate_inference_time_df","title":"<code>estimate_inference_time_df(*args, aggr=True, **kwargs)</code>","text":"<p>Same as <code>estimate_inference_time</code> but returns a <code>DataFrame</code> instead. Additional argument <code>aggr</code> can be set to <code>True</code> if only aggregations should be kept.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.estimate_inference_time_html","title":"<code>estimate_inference_time_html(file, *args, **kwargs)</code>","text":"<p>Same as <code>estimate_inference_time</code> but saves a <code>.html</code> file instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.estimate_inference_time_latex","title":"<code>estimate_inference_time_latex(*args, index=False, **kwargs)</code>","text":"<p>Same as <code>estimate_inference_time</code> but return a LaTeX output instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.estimate_ops","title":"<code>estimate_ops(module, input, pred_fn=None, eval=True)</code>","text":"<p>Estimates the number of operations computed in a forward pass of a module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>Input module.</p> required <code>input</code> <code>Union[Tensor, Tuple[Tensor]]</code> <p>Model input.</p> required <code>pred_fn</code> <code>Optional[Callable]</code> <p>Optional prediction function that replaces the forward call if the module requires additional steps.</p> <code>None</code> <code>eval</code> <code>bool</code> <p>If <code>True</code>, the module is set to eval mode before computing the inference time.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Results containing the estimated operations per module.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.estimate_ops_csv","title":"<code>estimate_ops_csv(file, *args, **kwargs)</code>","text":"<p>Same as <code>estimate_ops</code> but saves a <code>.csv</code> file instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.estimate_ops_df","title":"<code>estimate_ops_df(*args, **kwargs)</code>","text":"<p>Same as <code>estimate_ops</code> but returns a <code>DataFrame</code> instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.estimate_ops_html","title":"<code>estimate_ops_html(file, *args, **kwargs)</code>","text":"<p>Same as <code>estimate_ops</code> but saves a <code>.html</code> file instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.estimate_ops_latex","title":"<code>estimate_ops_latex(*args, index=False, **kwargs)</code>","text":"<p>Same as <code>estimate_ops</code> but returns a LaTeX output instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.estimate_total_inference_time","title":"<code>estimate_total_inference_time(module, input, eval=True, num_iters=1000, drop_first=100)</code>","text":"<p>Estimates the total inference time taken by the model to run an inference.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>Input module.</p> required <code>input</code> <code>Union[Tensor, Tuple[Tensor]]</code> <p>Model input.</p> required <code>eval</code> <code>bool</code> <p>If <code>True</code>, the module is set to eval mode before computing the inference time.</p> <code>True</code> <code>num_iters</code> <code>int</code> <p>Number of iterations to be performed.</p> <code>1000</code> <code>drop_first</code> <code>int</code> <p>Inferences to be dropped before aggregating the results.</p> <code>100</code> <p>Returns:</p> Type Description <code>dict</code> <p>Measurement results.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.estimate_total_inference_time_csv","title":"<code>estimate_total_inference_time_csv(file, *args, **kwargs)</code>","text":"<p>Same as <code>estimate_total_inference_time</code> but saves a <code>.csv</code> file instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.estimate_total_inference_time_df","title":"<code>estimate_total_inference_time_df(*args, aggr=False, **kwargs)</code>","text":"<p>Same as <code>estimate_total_inference_time</code> but returns a <code>DataFrame</code> instead. Additional argument <code>aggr</code> can be set to <code>True</code> if only aggregations should be kept.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.estimate_total_inference_time_html","title":"<code>estimate_total_inference_time_html(file, *args, **kwargs)</code>","text":"<p>Same as <code>estimate_total_inference_time</code> but saves a <code>.html</code> file instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.estimate_total_inference_time_latex","title":"<code>estimate_total_inference_time_latex(*args, index=False, **kwargs)</code>","text":"<p>Same as <code>estimate_total_inference_time</code> but returns a LaTeX output instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.trace_io_sizes","title":"<code>trace_io_sizes(module, input, pred_fn=None, eval=False)</code>","text":"<p>Traces the input and output tensor shapes of a module given a sample input.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>Input module.</p> required <code>input</code> <code>Union[torch.Tensor, Tuple[torch.Tensor]</code> <p>Model input.</p> required <code>pred_fn</code> <code>Optional[Callable]</code> <p>Optional prediction function that replaces the forward call if the module requires additional steps.</p> <code>None</code> <code>eval</code> <code>bool</code> <p>If <code>True</code>, the module is set to eval mode before computing the inference time.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Results containing input and output shapes of each module.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.trace_io_sizes_csv","title":"<code>trace_io_sizes_csv(file, *args, **kwargs)</code>","text":"<p>Same as <code>trace_io_sizes</code> but saves a <code>.csv</code> file instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.trace_io_sizes_df","title":"<code>trace_io_sizes_df(*args, **kwargs)</code>","text":"<p>Same as <code>trace_io_sizes</code> but returns a <code>DataFrame</code> instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.trace_io_sizes_html","title":"<code>trace_io_sizes_html(file, *args, **kwargs)</code>","text":"<p>Same as <code>trace_io_sizes</code> but saves a <code>.html</code> file instead.</p>"},{"location":"documentation/#moduleprofiler.profiler.ModuleProfiler.trace_io_sizes_latex","title":"<code>trace_io_sizes_latex(*args, index=False, **kwargs)</code>","text":"<p>Same as <code>trace_io_sizes</code> but returns a LaTeX output instead.</p>"},{"location":"license/","title":"License","text":"<p><code>moduleprofiler</code> was developed by Esteban G\u00f3mez, part of the Speech Interaction Technology group from Aalto University and is licensed under CC BY-SA 4.0.</p> <p>If this package contributed to your work, please consider citing it:</p> <pre><code>@misc{moduleprofiler,\n  author = {Esteban G\u00f3mez},\n  title  = {moduleprofiler},\n  year   = 2024,\n  url    = {https://github.com/eagomez2/moduleprofiler}\n}\n</code></pre>"},{"location":"reference/","title":"Reference","text":"<p>This complementary section explains the derivation of the formulas used by <code>moduleprofiler</code> to estimate the complexity of some neural network modules. It is intended to be used both as a quick reference and for educational purposes.</p> <p>Note</p> <p>It is important to consider that all formulas derived in this section are purely based on the mathematical relationship between the input and output of each module. In practice, there could be additional optimizations performed by linear algebra libraries or hardware-specific capabilities that are used to accelerate or minimize the calculations required by a specific module type to compute the corresponding output. For this reason, estimating complexity using <code>moduleprofiler</code> may result in bigger numbers compared to other packages used for similar purposes.</p> <p>List of reference pages:</p> <ul> <li>Conv1d (<code>torch.nn.Conv1d</code>)</li> <li>Conv2d (<code>torch.nn.Conv2d</code>)</li> <li>ConvTranspose1d (<code>torch.nn.ConvTranspose1d</code>)</li> <li>ConvTranspose2d (<code>torch.nn.ConvTranspose2d</code>)</li> <li>GRUCell (<code>torch.nn.GRUCell</code>)</li> <li>GRU (<code>torch.nn.GRU</code>)</li> <li>LayerNorm (<code>torch.nn.LayerNorm</code>)</li> <li>Linear (<code>torch.nn.Linear</code>)</li> <li>LSTMCell (<code>torch.nn.LSTMCell</code>)</li> <li>LSTM (<code>torch.nn.LSTM</code>)</li> <li>ReLU (<code>torch.nn.ReLU</code>)</li> <li>Sigmoid (<code>torch.nn.Sigmoid</code>)</li> <li>Softmax (<code>torch.nn.Softmax</code>)</li> <li>Tanh (<code>torch.nn.Tanh</code>)</li> </ul>"},{"location":"tutorial/","title":"Tutorial","text":"<p>This tutorial covers everything you need to know in order to use <code>moduleprofiler</code>. It begins with an introduction and explanation of the most basic methods, followed by a section on extending the calculations to your own custom <code>torch.nn.Module</code> modules. After completing this tutorial, you should be able to take advantage of this package and start using it in your own projects.</p>"},{"location":"tutorial/#introduction","title":"Introduction","text":"<p><code>moduleprofiler</code> is a free open-source package to profile <code>torch.nn.Module</code> modules and obtain useful information to design a model that fits your needs and constraints at development time. With <code>moduleprofiler</code> you can calculate the number of parameters of your model, trace the input and output sizes of each layer, estimate the number of operations performed by each layer in the forward pass, and calculate the inference time. The result of any profiling task can be obtained as a <code>dict</code> or <code>pandas.DataFrame</code> for further manipulation, can be exported to <code>.html</code> and <code>.csv</code> files, and can be used to generated a <code>LaTeX</code> table that you can add to your publication.</p> <p><code>moduleprofiler</code> works by iterating through all your module's <code>torch.nn.Module</code> instances. It uses a varied set of hooks that allows it to collect information abot your model. These hooks are temporarily added and then removed after performing all calculations. For this reason, <code>moduleprofiler</code> can work with any <code>torch.nn.Module</code> without needing any additional line of code, and without performing any permanent changes to your original model.</p> <p>It comes equipped with all the necessary methods to deal with the most commonly used set of layers. However, you can also your own custom layers or modify the existing calculations to fit your particular needs.</p>"},{"location":"tutorial/#basic-usage","title":"Basic usage","text":""},{"location":"tutorial/#installation","title":"Installation","text":"<p><code>moduleprofiler</code> can be installed as any regular <code>python</code> module within your environment:</p> <p>Install from PyPI: <pre><code>python -m pip install moduleprofiler\n</code></pre></p> <p>Install from this repository: <pre><code>python -m pip install git+https://github.com/eagomez2/moduleprofiler.git\n</code></pre></p> <p>After doing this, all dependencies should be automatically resolved and you should be ready to start using it. In case of doubts about dependency version, you can inspect the <code>pyproject.toml</code> file in the root of this package.</p>"},{"location":"tutorial/#basic-profiling","title":"Basic profiling","text":"<p>All the basic functionality of <code>moduleprofiler</code> (and the most frequently required to profile a model) is encapsulated in a single class called <code>ModuleProfiler</code>.</p> <pre><code>import torch\nfrom moduleprofiler import ModuleProfiler\n\nprofiler = ModuleProfiler()\n</code></pre> <p>By default, a <code>ModuleProfiler</code> instance will be equipped with all you need, therefore no extra arguments are required, although you can inspect the available options in the Documentation section.</p> <p>After creating the required instance, you can already start profiling your module using one of the following methods:</p> <ul> <li><code>profiler.count_params</code></li> <li><code>profiler.estimate_inference_time</code></li> <li><code>profiler.estimate_ops</code></li> <li><code>profiler.estimate_total_inference_time</code></li> <li><code>profiler.trace_io_sizes</code></li> </ul> <p>All these methods are available with different suffixes depending on the expected output format. For example, <code>profiler.count_params</code> will output a <code>dict</code> with all information about parameters in your module. On the other hand, <code>profiler.count_params_df</code> will output a <code>pandas.DataFrame</code> instead.</p> <pre><code>import torch\nfrom moduleprofiler import ModuleProfiler\n\n# Profiler \nprofiler = ModuleProfiler()\n\n# Network\nnet = torch.nn.Linear(in_features=8, out_features=32)\n\n# Number of parameters with dict output\nparams_dict = profiler.count_params(module=net)\n\"\"\"\n{'__root__':\n    {\n        'type': 'Linear',\n        'trainable_params': 288,\n        'nontrainable_params': 0,\n        'trainable_params_dtype': torch.float32, \n        'trainable_params_size_bits': 9216,\n        'trainable_params_percent': 1.0,\n        'nontrainable_params_percent': 0.0\n    }\n}\n\"\"\"\n\n# Number of parameters with DataFrame output\nparams_df = profiler.count_params_df(module=net)\n\"\"\"\n     module    type  trainable_params  nontrainable_params trainable_params_dtype  trainable_params_size_bits  trainable_params_percent  nontrainable_params_percent\n0  __root__  Linear               288                    0          torch.float32                        9216                       1.0                          0.0\n\"\"\"\n</code></pre> <p>All results are consistent across formats, so it should be trivial to interpret them knowing the basics. You may see a <code>__root__</code> module in some of your results. This represents top-level <code>torch.nn.Module</code>.</p> <p>Some methods such as <code>profiler.trace_io_sizes</code> or <code>profiler.estimate_ops</code> will require an example tensor when called, because the sizes of layer inputs and outputs will depend on this.</p> <pre><code># Example tensor\nx = torch.rand((1, 8))\n\n# Input and output sizes\nio_sizes_df = profiler.trace_io_sizes_df(module=net, input=x)\n\"\"\"\n     module    type input_size output_size\n0  __root__  Linear     (1, 8)     (1, 32)\n\"\"\"\n\n# Operations\nops_df = profiler.estimate_ops_df(module=net, input=x)\n\"\"\"\n     module    type  ops\n0  __root__  Linear  512\n\"\"\"\n</code></pre> <p>The complete code of this section is shown below:</p> basic_profiling.py<pre><code>import torch\nfrom moduleprofiler import ModuleProfiler\n\n# Profiler \nprofiler = ModuleProfiler()\n\n# Network\nnet = torch.nn.Linear(in_features=8, out_features=32)\n\n# Number of parameters with dict output\nparams_dict = profiler.count_params(module=net)\n\"\"\"\n{'__root__':\n    {\n        'type': 'Linear',\n        'trainable_params': 288,\n        'nontrainable_params': 0,\n        'trainable_params_dtype': torch.float32, \n        'trainable_params_size_bits': 9216,\n        'trainable_params_percent': 1.0,\n        'nontrainable_params_percent': 0.0\n    }\n}\n\"\"\"\n\n# Number of parameters with DataFrame output\nparams_df = profiler.count_params_df(module=net)\n\"\"\"\n     module    type  trainable_params  nontrainable_params trainable_params_dtype  trainable_params_size_bits  trainable_params_percent  nontrainable_params_percent\n0  __root__  Linear               288                    0          torch.float32                        9216                       1.0                          0.0\n\"\"\"\n\n# Example tensor\nx = torch.rand((1, 8))\n\n# Input and output sizes\nio_sizes_df = profiler.trace_io_sizes_df(module=net, input=x)\n\"\"\"\n     module    type input_size output_size\n0  __root__  Linear     (1, 8)     (1, 32)\n\"\"\"\n\n# Operations\nops_df = profiler.estimate_ops_df(module=net, input=x)\n\"\"\"\n     module    type  ops\n0  __root__  Linear  512\n\"\"\"\n</code></pre>"},{"location":"tutorial/#inference-time","title":"Inference time","text":"<p>Estimating inference time works as follows: First, your model is run multiple times and the time taken during each iteration is stored. After all runs are completed, some of the first iterations are dropped since these are typically slower than subsequent executions. This may be due to multiple reasons such as layer warm-up, benchmarking or caching mechanisms. By default, <code>1000</code> iterations are performed and the first <code>100</code> are dropped, but this number can configured by the user using the <code>num_iters</code> and <code>drop_first</code> parametrers. After this, only the relevant information is kept and all aggregations reported in the final output are based on these inference times.</p> <pre><code>import torch\nfrom moduleprofiler import ModuleProfiler\n\n# Profiler \nprofiler = ModuleProfiler()\n\n# Network\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=8, out_features=32),\n    torch.nn.Sigmoid()\n)\n\n# Example tensor\nx = torch.rand((1, 8))\n\n# Compute inference time (aggregated)\naggr_inference_time_df = profiler.estimate_inference_time_df(module=net, input=x, num_iters=1000, drop_first=100)\n\"\"\"\n     module        type  inference_time_mean_ms  inference_time_max_ms  inference_time_min_ms  inference_time_std_ms  ...           host_name      os  os_release cpu_count total_memory          gpu\n0  __root__  Sequential                0.008688               0.216167               0.007917               0.006605  ...  xxxxxxxxxxxx.local  Darwin      23.4.0        12     18432 MB  unavailable\n1         0      Linear                0.003139               0.109833               0.002791               0.003388  ...  xxxxxxxxxxxx.local  Darwin      23.4.0        12     18432 MB  unavailable\n2         1     Sigmoid                0.001511               0.028208               0.001333               0.000852  ...  xxxxxxxxxxxx.local  Darwin      23.4.0        12     18432 MB  unavailable\n\"\"\"\n\n# Compute inference time\ninference_time_df = profiler.estimate_inference_time_df(module=net, input=x, num_iters=1000, drop_first=100, aggr=False)\n\"\"\"\n        module        type inference_time_ms  intraop_threads  interop_threads           host_name      os os_release  cpu_count total_memory          gpu\n0     __root__  Sequential          0.040958                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n1     __root__  Sequential             0.011                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n2     __root__  Sequential          0.010333                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n3     __root__  Sequential          0.009791                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n4     __root__  Sequential          0.009916                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n...        ...         ...               ...              ...              ...                 ...     ...        ...        ...          ...          ...\n2995         1     Sigmoid          0.001584                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n2996         1     Sigmoid          0.001584                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n2997         1     Sigmoid          0.001541                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n2998         1     Sigmoid          0.001583                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n2999         1     Sigmoid          0.001542                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n\"\"\"\n</code></pre> <p>Please note that there is some extra information besides the aggregated statistics of the inference time of your model. Specifications such as <code>host_name</code>, <code>os_release</code> (operating system version) and <code>cpu_count</code> (total number of cores) is added to the results.</p> <p>If instead of aggregated statistics you want to obtain the time taken by each module during each inference, you can set <code>aggr=False</code> in <code>profiler.estimate_inference_time_df</code>. In this case, the first <code>drop_first</code> will also be included in the results and you will obtain a <code>DataFrame</code> with <code>num_iters * num_layers</code> rows.</p> <p>Warning</p> <p>Please note that the inference of time of your model is hardware-dependent. This means that you may get significantly different values if you run the same model in different devices. Additionally, you may get significantly different numbers depending on the amount of configured intra-op and inter-op threads. To configure these you can use <code>torch.get_num_threads()</code>, <code>torch.set_num_threads()</code>, <code>torch.get_num_interop_threads()</code> and <code>torch.set_num_interop_threads()</code>. For further details about these methods, please refer to PyTorch documentation.</p> <p>In addition to <code>profiler.estimate_inference_time</code> or <code>profiler.estimate_inference_time_df</code> there is a <code>profiler.estimate_total_inference_time</code> and <code>profiler.estimate_total_inference_time_df</code> method. This method represents a shortcut to obtaining statistics for the whole model, rather than computing them layer by layer. The method is recommended when you only need to compute the inference time for the whole model rather than layer by layer. Similarly to <code>profiler.estimate_inference_time</code>, you can compute individual inferences or aggregated metrics using the <code>aggr</code> parameter.</p> <pre><code># Compute inference time (aggregated)\naggr_total_inference_time_df = profiler.estimate_total_inference_time_df(module=net, input=x, num_iters=1000, drop_first=100, aggr=True)\n\"\"\"\n     module        type  intraop_threads  interop_threads           host_name  ... inference_time_mean_ms inference_time_min_ms  inference_time_max_ms inference_time_std_ms inference_time_median_ms\n0  __root__  Sequential                6               12  xxxxxxxxxxxx.local  ...               0.004034              0.003833               0.011959              0.000361                    0.004\n\"\"\"\n\n# Compute inference time\ntotal_inference_time_df = profiler.estimate_total_inference_time_df(module=net, input=x, num_iters=1000, drop_first=100)\n\"\"\"\n       module        type  intraop_threads  interop_threads           host_name      os os_release  cpu_count total_memory          gpu inference_time_ms\n0    __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable             0.004\n1    __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable          0.004041\n2    __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable             0.004\n3    __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable             0.004\n4    __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable          0.004042\n..        ...         ...              ...              ...               ...     ...        ...        ...          ...          ...               ...\n895  __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable          0.005375\n896  __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable          0.005375\n897  __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable          0.004958\n898  __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable          0.005125\n899  __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable           0.00525\n\"\"\"\n</code></pre> <p>The complete code of this section is shown below:</p> inference_time.py<pre><code>import torch\nfrom moduleprofiler import ModuleProfiler\n\n# Profiler \nprofiler = ModuleProfiler()\n\n# Network\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=8, out_features=32),\n    torch.nn.Sigmoid()\n)\n\n# Example tensor\nx = torch.rand((1, 8))\n\n# Compute inference time (aggregated)\naggr_inference_time_df = profiler.estimate_inference_time_df(module=net, input=x, num_iters=1000, drop_first=100)\n\"\"\"\n     module        type  inference_time_mean_ms  inference_time_max_ms  inference_time_min_ms  inference_time_std_ms  ...           host_name      os  os_release cpu_count total_memory          gpu\n0  __root__  Sequential                0.008688               0.216167               0.007917               0.006605  ...  xxxxxxxxxxxx.local  Darwin      23.4.0        12     18432 MB  unavailable\n1         0      Linear                0.003139               0.109833               0.002791               0.003388  ...  xxxxxxxxxxxx.local  Darwin      23.4.0        12     18432 MB  unavailable\n2         1     Sigmoid                0.001511               0.028208               0.001333               0.000852  ...  xxxxxxxxxxxx.local  Darwin      23.4.0        12     18432 MB  unavailable\n\"\"\"\n\n# Compute inference time\ninference_time_df = profiler.estimate_inference_time_df(module=net, input=x, num_iters=1000, drop_first=100, aggr=False)\n\"\"\"\n        module        type inference_time_ms  intraop_threads  interop_threads           host_name      os os_release  cpu_count total_memory          gpu\n0     __root__  Sequential          0.040958                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n1     __root__  Sequential             0.011                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n2     __root__  Sequential          0.010333                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n3     __root__  Sequential          0.009791                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n4     __root__  Sequential          0.009916                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n...        ...         ...               ...              ...              ...                 ...     ...        ...        ...          ...          ...\n2995         1     Sigmoid          0.001584                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n2996         1     Sigmoid          0.001584                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n2997         1     Sigmoid          0.001541                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n2998         1     Sigmoid          0.001583                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n2999         1     Sigmoid          0.001542                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable\n\"\"\"\n\n# Compute inference time (aggregated)\naggr_total_inference_time_df = profiler.estimate_total_inference_time_df(module=net, input=x, num_iters=1000, drop_first=100, aggr=True)\n\"\"\"\n     module        type  intraop_threads  interop_threads           host_name  ... inference_time_mean_ms inference_time_min_ms  inference_time_max_ms inference_time_std_ms inference_time_median_ms\n0  __root__  Sequential                6               12  xxxxxxxxxxxx.local  ...               0.004034              0.003833               0.011959              0.000361                    0.004\n\"\"\"\n\n# Compute inference time\ntotal_inference_time_df = profiler.estimate_total_inference_time_df(module=net, input=x, num_iters=1000, drop_first=100)\n\"\"\"\n       module        type  intraop_threads  interop_threads           host_name      os os_release  cpu_count total_memory          gpu inference_time_ms\n0    __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable             0.004\n1    __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable          0.004041\n2    __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable             0.004\n3    __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable             0.004\n4    __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable          0.004042\n..        ...         ...              ...              ...               ...     ...        ...        ...          ...          ...               ...\n895  __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable          0.005375\n896  __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable          0.005375\n897  __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable          0.004958\n898  __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable          0.005125\n899  __root__  Sequential                6               12  xxxxxxxxxxxx.local  Darwin     23.4.0         12     18432 MB  unavailable           0.00525\n\"\"\"\n</code></pre>"},{"location":"tutorial/#running-documentation","title":"Running documentation","text":"<p>Besides the available online documentation, you can run your own copy locally. This might be helpful for quicker inspection or while you are working offline. After installing all dependencies, simply go to the root of this package and run</p> <pre><code>mkdocs serve\n</code></pre> <p>If everything run correctly, you should see a message in your terminal containing the URL where the documentation will be served.</p> <pre><code>INFO    -  Building documentation...\nINFO    -  Cleaning site directory\nINFO    -  Documentation built in 0.39 seconds\nINFO    -  [17:14:19] Watching paths for changes: 'docs', 'mkdocs.yml'\nINFO    -  [17:14:19] Serving on http://127.0.0.1:8000/\n</code></pre> <p>Finally, just open the URL shown in the screen in your browser of choice.</p>"},{"location":"tutorial/#running-unit-tests","title":"Running unit tests","text":"<p><code>moduleprofiler</code> relies on <code>pytest</code> for running unit tests. All available tests are in the <code>tests</code> folder. To run all tests from the terminal, simpy enable your python environment, go to the package's root and run</p> <pre><code>pytest -v\n</code></pre> <p>This command will automatically detect and collect all test and run them. The results will be displayed in your terminal. </p>"},{"location":"tutorial/#advanced-usage","title":"Advanced usage","text":"<p><code>moduleprofiler</code> comes batteries-included. It already has supports for all the most common <code>torch.nn.Module</code> types. However, there are cases where it is necessary to add custom calculations, such as when you create a custom <code>torch.nn.Module</code> and want to include it in the estimations <code>moduleprofiler</code> can provide. The next sections explain how to add your custom module to both ops estimation and io size tracing. </p>"},{"location":"tutorial/#extending-ops-estimation","title":"Extending ops estimation","text":"<p><code>moduleprofiler</code> uses a <code>dict</code> that maps <code>torch.nn.Module</code> types to a function that is called when the number of operations are computed. The function signature is always as follows:</p> <pre><code>def ops_fn(module: torch.nn.Module, input: Tuple[torch.Tensor], output: torch.Tensor) -&gt; int:\n     # Your function body\n     ...\n</code></pre> <p>Note</p> <p>Please note that this is the same signature as PyTorch forward hooks. If the input to the <code>forward</code> method of your module is a single <code>torch.Tensor</code>, the function will receive a <code>tuple</code> of one item.</p> <p>To include your custom module (or overwrite existing calculations), you simply need to get the default <code>dict</code>, add your custom mapping, and pass it to the <code>ModuleProfiler</code> instance.</p> custom_module_ops.py<pre><code>import torch\nfrom typing import Tuple\nfrom moduleprofiler import (\n     ModuleProfiler,\n     get_default_ops_map\n)\n\n\nclass PlusOne(torch.nn.Module):\n    \"\"\"A custom module that adds one to all items in the input tensor.\"\"\"\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return x + 1.0\n\n\ndef plusone_ops_fn(\n        module: PlusOne,\n        input: Tuple[torch.Tensor],\n        output: torch.Tensor\n) -&gt; int:\n    # One operation per item\n    return input[0].numel()\n\n# Get default ops map\ncustom_ops_fn_map = get_default_ops_map()\n\n# Register new function\ncustom_ops_fn_map[PlusOne] = plusone_ops_fn\n\n# Create profiler with updated ops map\nprofiler = ModuleProfiler(ops_fn_map=custom_ops_fn_map)\n\n# Create network with custom module\nnet = PlusOne()\n\n# Create example tensor\nx = torch.rand((1, 8))\n\n# Compute ops\nops_df = profiler.estimate_ops_df(module=net, input=x)\n\"\"\"\n     module     type  ops\n0  __root__  PlusOne    8\n\"\"\"\n</code></pre>"},{"location":"tutorial/#conclusion","title":"Conclusion","text":"<p>Congratulations! / \u00a1Felicidades! / Felicitats! / Onnittelut! Now that you have completed this tutorial, you should be ready to take advantage of <code>moduleprofiler</code> in your own projects.</p>"},{"location":"modules/conv1d/","title":"Conv1d (<code>torch.nn.Conv1d</code>)","text":"<p>A <code>torch.nn.Conv1d</code> module applies the cross-correlation operation along a given dimension of a tensor. This may seem contradictory at first, because the module's name implies that the underlying operation should be convolution, yet both operations are similar.</p> <p>Note</p> <p>Please note that the cross-correlation operation \\(\\star\\) is used instead of convolution \\(\\ast\\) even when the module name suggests the opposite. The main difference between these two operations is the kernel \\(g\\) ordering, but the number of computations are equivalent. For this reason, we will use the term cross-correlation and convolution interchangeably hereafter. $$     \\left(f\\ast g\\right)[n]=\\sum\\limits_{k=0}^{K-1}f[n]\\times g[n-k]\\qquad \\left(\\text{convolution}\\right)\\\\~\\\\     \\left(f\\star g\\right)[n]=\\sum\\limits_{k=0}^{K-1}f[n]\\times g[n+k]\\qquad \\left(\\text{cross-correlation}\\right) $$</p> <p>A <code>torch.nn.Conv1d</code> module expects an input of size \\(\\left(N,C_{\\text{in}}, L_{\\text{in}}\\right)\\) or \\(\\left(C_{\\text{in}}, L_{\\text{in}}\\right)\\) to produce an output of size \\(\\left(N,C_{\\text{out}}, L_{\\text{out}}\\right)\\) or \\(\\left(C_{\\text{out}}, L_{\\text{out}}\\right)\\) performing the following operation</p> \\[ \\begin{equation}     \\text{out}\\left(N_i, C_{\\text{out}_j}\\right) = \\text{bias}\\left(C_{\\text{out}_j}\\right) + \\sum\\limits_{k=0}^{C_{\\text{in}}-1}\\text{weight}\\left(C_{\\text{out}_j}, k\\right) \\star \\text{input}\\left(N_i, k\\right) \\end{equation} \\] <p>Where</p> <ul> <li>\\(N\\) is the batch size.</li> <li>\\(C_{\\text{in}}\\) is the number of input channels.</li> <li>\\(C_{\\text{out}}\\) is the number of output channels.</li> <li>\\(L_{\\text{in}}\\) is the length of the input tensor (i.e. <code>x.size(-1)</code> assuming an input tensor <code>x</code>).</li> <li>\\(L_{\\text{out}}\\) is the length of the output tensor (i.e. <code>y.size(-1)</code> assuming an output tensor <code>y</code>).</li> <li>\\(\\star\\) is the cross-correlation operator.</li> </ul> <p>Additionally, \\(L_{\\text{out}}\\) (<code>y.size(-1)</code>) will depend on \\(L_{\\text{in}}\\) (<code>x.size(-1)</code>), <code>padding</code>, <code>dilation</code>, <code>kernel_size</code> and <code>stride</code> parameters. The relationship between these can be expressed as</p> \\[ \\begin{equation} L_\\text{out}=\\left[\\frac{L_{\\text{in}}+2\\times\\text{padding} - \\text{dilation}\\times\\left(\\text{kernel\\_size} - 1\\right) - 1}{\\text{stride}}+1\\right] \\end{equation} \\]"},{"location":"modules/conv1d/#complexity","title":"Complexity","text":""},{"location":"modules/conv1d/#number-of-filters","title":"Number of filters","text":"<p>In order to calculate the number of operations performed this module, it is necessary to understand the impact of the <code>groups</code> parameter on the overall complexity, and the number of filters \\(\\psi\\) a network instance will have based on this. According to the official <code>torch.nn.Conv1d</code> documentation</p> <p><code>groups</code>\u00a0controls the connections between inputs and outputs.\u00a0<code>in_channels</code>\u00a0and\u00a0<code>out_channels</code>\u00a0must both be divisible by\u00a0<code>groups</code>.</p> <p>For example: At <code>groups=1</code>, all inputs are convolved to all outputs. At <code>groups=2</code>, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.</p> <p>At <code>groups=in_channels</code>, each input channel is convolved with its own set of filters (of size \\(\\frac{\\text{out\\_channels}}{\\text{in\\_channels}}\\)\u00a0) </p> <p>Based on this information, the number of filters \\(\\psi\\) can be computed as</p> \\[ \\begin{equation} \\psi = \\left(\\frac{C_{\\text{in}}}{\\text{groups}}\\right)\\times\\left(\\frac{C_{\\text{out}}}{\\text{groups}}\\right)\\times{\\text{groups}}=\\frac{C_{\\text{in}}\\times C_{\\text{out}}}{\\text{groups}} \\end{equation} \\]"},{"location":"modules/conv1d/#operations-per-filter","title":"Operations per filter","text":"<p>Now the number of filters \\(\\psi\\) are known, it is necessary to compute how many operations each filter performs. As shown in Figure 1, for each kernel position there will be \\(\\text{kernel\\_size}\\) multiplications (i.e. each kernel element multiplied by a slice of the input tensor of the same size) and \\(\\text{kernel\\_size}-1\\) additions to aggregate the result and obtain one element of the output.</p> Figure 1. Operations per kernel position to obtain the output tensor. <p>Since each element in \\(L_\\text{out}\\) is the result of the operations carried out for a single kernel position, the number of operations per filter \\(\\lambda\\) can be expressed as</p> \\[ \\begin{equation}     \\lambda=L_{\\text{out}}\\times\\left(\\text{kernel\\_size}+\\left(\\text{kernel\\_size}-1\\right)\\right)=L_{\\text{out}}\\times\\left(2\\times\\text{kernel\\_size}-1\\right) \\end{equation} \\] <p>Note</p> <p>Please note that the batch size \\(N\\) will be ignored for now, but it will be included later on.</p>"},{"location":"modules/conv1d/#filter-aggregation","title":"Filter aggregation","text":"<p>Now that the number of filters and the number of operations per filter are known, it is necessary compute the operations needed to aggregate each group of filters \\(\\gamma\\) to produce each output channel \\(C_\\text{out}\\). These operations correspond to simple element-wise additions and can be expressed as</p> \\[ \\begin{equation} \\gamma=C_{\\text{out}}\\times L_\\text{out}\\times\\left(\\left(\\frac{C_{\\text{in}}}{\\text{groups}}-1\\right)+1\\right) \\end{equation} \\] <p>Where the term \\(\\left(\\frac{C_{\\text{in}}}{\\text{groups}}-1\\right)\\) corresponds to the number of grouped connections between input and outputs channels \\(\\frac{C_{\\text{in}}}{\\text{groups}}\\), subtracted by \\(1\\) because the operation is an addition. The \\(L_\\text{out}\\) factor accounts for the number of elements per filters, and \\(C_{\\text{out}}\\) expands this calculation to all output channels. Finally, the remaining \\(+1\\) corresponds to the bias term \\(b\\) that was not included so far, and that is added to each resulting output channel element. Note that this last term is only added if the module is instantiated using <code>bias=True</code>.</p> \\[ \\begin{equation} \\gamma=\\begin{cases}     C_{\\text{out}}\\times L_\\text{out}\\times\\left(\\frac{C_{\\text{in}}}{\\text{groups}}\\right), &amp; \\text{if}\\ \\text{bias}=\\text{True} \\\\     C_{\\text{out}}\\times L_\\text{out}\\times\\left(\\frac{C_{\\text{in}}}{\\text{groups}}-1\\right), &amp; \\text{if}\\ \\text{bias}=\\text{False} \\end{cases} \\end{equation} \\] <p>Note</p> <p>Please note that the bias term \\(b\\) was not included in  Operations per filter and is added here instead. Even though according to PyTorch <code>torch.nn.Conv1d</code> documentation \\(b\\) has shape \\(\\left(C_\\text{out}\\right)\\), in practice this tensor is implicitly broadcasted following PyTorch broadcasting semantics in such a way that each tensor value will be added with its corresponding channel bias.</p>"},{"location":"modules/conv1d/#total-operations","title":"Total operations","text":"<p>Now putting together all different factors that contribute to the total number of operations as well as including the batch size \\(N\\)</p> \\[ \\begin{equation}     \\text{Conv1d}_{ops}=N\\times\\left(\\psi\\times\\lambda+\\gamma\\right) \\end{equation} \\] <p>Where</p> <ul> <li>\\(N\\) is the batch size.</li> <li>\\(\\psi\\) is the number of filters.</li> <li>\\(\\lambda\\) is the number of operations per filter.</li> <li>\\(\\gamma\\) is the number of filter aggregation operations.</li> </ul> <p>For the case of <code>bias=True</code> this can be expanded to</p> \\[ \\begin{equation}     \\small{\\text{Conv1d}_{ops}=N\\times\\left(\\left(\\frac{C_{\\text{in}}\\times C_{\\text{out}}}{\\text{groups}}\\right)\\times\\left(L_{\\text{out}}\\times\\left(2\\times\\text{kernel\\_size}-1\\right)\\right)+C_{\\text{out}}\\times L_\\text{out}\\times\\left(\\frac{C_{\\text{in}}}{\\text{groups}}\\right)\\right)} \\end{equation} \\] <p>Rearranging terms it can be simplified to</p> \\[ \\begin{equation}     \\text{Conv1d}_{ops}=N\\times\\left(\\frac{C_{\\text{in}}\\times C_{\\text{out}}\\times L_{\\text{out}}\\times2\\times\\text{kernel\\_size}}{\\text{groups}}\\right) \\end{equation} \\] <p>For the case of <code>bias=False</code> \\(\\gamma=C_{\\text{out}}\\times L_\\text{out}\\times\\left(\\frac{C_{\\text{in}}}{\\text{groups}}-1\\right)\\)  and the whole expression can be simplified to</p> \\[ \\begin{equation} \\text{Conv1d}_{ops}=N\\times\\left(\\frac{C_{\\text{out}}\\times L_{\\text{out}}\\times\\left( C_\\text{in}\\times2\\times\\text{kernel\\_size}-\\text{groups}\\right)}{\\text{groups}}\\right) \\end{equation} \\]"},{"location":"modules/conv1d/#summary","title":"Summary","text":"<p>The number of operations performed by a <code>torch.nn.Conv1d</code> module can be estimated as</p> If <code>bias=True</code>If <code>bias=False</code> <p>\\(\\large{\\text{Conv1d}_{ops}=N\\times\\left(\\frac{C_{\\text{in}}\\times C_{\\text{out}}\\times L_{\\text{out}}\\times2\\times\\text{kernel\\_size}}{\\text{groups}}\\right)}\\)</p> <p>\\(\\large{\\text{Conv1d}_{ops}=N\\times\\left(\\frac{C_{\\text{out}}\\times L_{\\text{out}}\\times\\left( C_\\text{in}\\times2\\times\\text{kernel\\_size} - \\text{groups}\\right)}{\\text{groups}}\\right)}\\)</p> <p>Where</p> <ul> <li>\\(N\\) is the batch size.</li> <li>\\(C_{\\text{in}}\\) is the number of input channels.</li> <li>\\(C_{\\text{out}}\\) is the number of output channels.</li> <li>\\(L_{\\text{out}}\\) is the length of the output tensor (i.e. <code>y.size(-1)</code> assuming an output tensor <code>y</code>).</li> <li>\\(\\text{kernel\\_size}\\) is the length of the kernel.</li> <li>\\(\\text{groups}\\) is the number of groups.</li> </ul>"},{"location":"modules/conv2d/","title":"Conv2d (<code>torch.nn.Conv2d</code>)","text":"<p>A <code>torch.nn.Conv2d</code> module applies the cross-correlation operation along a given pair of dimensions of a tensor. This may seem contradictory at first, because the module's name implies that the underlying operation should be convolution, yet both operations are similar.</p> <p>Note</p> <p>Please note that the cross-correlation operation \\(\\star\\) is used instead of convolution \\(\\ast\\) even when the module name suggests the opposite. The main difference between these two operations is the kernel \\(g\\) ordering, but the number of computations are equivalent. For this reason, we will use the term cross-correlation and convolution interchangeably hereafter. $$     \\left(f\\ast g\\right)[n]=\\sum\\limits_{k=0}^{K-1}f[n]\\times g[n-k]\\qquad \\left(\\text{convolution}\\right)\\\\~\\\\     \\left(f\\star g\\right)[n]=\\sum\\limits_{k=0}^{K-1}f[n]\\times g[n+k]\\qquad \\left(\\text{cross-correlation}\\right) $$</p> <p>A <code>nn.Conv2d</code> module expects an input of size \\(\\left(N,C_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}}\\right)\\) to produce an output of size \\(\\left(N,C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}}\\right)\\) performing the following operation</p> \\[ \\begin{equation}     \\text{out}\\left(N_i, C_{\\text{out}_j}\\right) = \\text{bias}\\left(C_{\\text{out}_j}\\right) + \\sum\\limits_{k=0}^{C_{\\text{in}}-1}\\text{weight}\\left(C_{\\text{out}_j}, k\\right) \\star \\text{input}\\left(N_i, k\\right) \\end{equation} \\] <p>Where</p> <ul> <li>\\(N\\) is the batch size.</li> <li>\\(C_{\\text{in}}\\) is the number of input channels.</li> <li>\\(C_{\\text{out}}\\) is the number of output channels.</li> <li>\\(H_{\\text{in}}\\) is the height of the input tensor (i.e. <code>x.size(-2)</code> assuming an input tensor <code>x</code>)</li> <li>\\(W_{\\text{in}}\\) is the width of the input tensor (i.e. <code>x.size(-1)</code> assuming an input tensor <code>x</code>)</li> <li>\\(H_{\\text{out}}\\) is the height of the output tensor (i.e. <code>y.size(-2)</code> assuming an output tensor <code>y</code>)</li> <li>\\(W_{\\text{out}}\\) is the width of the output tensor (i.e. <code>y.size(-1)</code> assuming an output tensor <code>y</code>)</li> <li>\\(\\star\\) is the cross-correlation operator.</li> </ul> <p>Additionally, \\(H_{\\text{out}}\\) (<code>y.size(-2)</code>) and \\(W_{\\text{out}}\\) (<code>y.size(-1)</code>)  will depend on \\(H_{\\text{in}}\\) (<code>x.size(-2)</code>), \\(W_{\\text{in}}\\) (<code>x.size(-1)</code>), <code>padding</code>, <code>dilation</code>, <code>kernel_size</code> and <code>stride</code> parameters. The relationship between these can be expressed as</p> \\[ \\begin{equation}     H_\\text{out}=\\left[\\frac{H_{\\text{in}}+2\\times\\text{padding[0]} - \\text{dilation[0]}\\times\\left(\\text{kernel\\_size[0]} - 1\\right) - 1}{\\text{stride[0]}}+1\\right] \\\\ \\end{equation} \\] \\[ \\begin{equation}     W_\\text{out}=\\left[\\frac{W_{\\text{in}}+2\\times\\text{padding[1]} - \\text{dilation[1]}\\times\\left(\\text{kernel\\_size[1]} - 1\\right) - 1}{\\text{stride[1]}}+1\\right] \\end{equation} \\] <p>Where indices \\(\\text{[0]}\\) and \\(\\text{[1]}\\) indicate first and second element of each tuple, respectively, since padding, dilation and kernel_size are now specified as tuples of two elements, compared to the nn.Conv1d case where they correspond to a single integer number.</p>"},{"location":"modules/conv2d/#complexity","title":"Complexity","text":""},{"location":"modules/conv2d/#number-of-filters","title":"Number of filters","text":"<p>In order to calculate the number of operations performed this module, it is necessary to understand the impact of the <code>groups</code> parameter on the overall complexity, and the number of filters \\(\\psi\\) a network instance will have based on this. According to the official <code>torch.nn.Conv1d</code> documentation</p> <p><code>groups</code>\u00a0controls the connections between inputs and outputs.\u00a0<code>in_channels</code>\u00a0and\u00a0<code>out_channels</code>\u00a0must both be divisible by\u00a0<code>groups</code>.</p> <p>For example: At <code>groups=1</code>, all inputs are convolved to all outputs. At <code>groups=2</code>, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.</p> <p>At <code>groups=in_channels</code>, each input channel is convolved with its own set of filters (of size \\(\\frac{\\text{out\\_channels}}{\\text{in\\_channels}}\\)\u00a0) </p> <p>Based on this information, the number of filters \\(\\psi\\) can be computed as</p> \\[ \\begin{equation} \\psi = \\left(\\frac{C_{\\text{in}}}{\\text{groups}}\\right)\\times\\left(\\frac{C_{\\text{out}}}{\\text{groups}}\\right)\\times{\\text{groups}}=\\frac{C_{\\text{in}}\\times C_{\\text{out}}}{\\text{groups}} \\end{equation} \\]"},{"location":"modules/conv2d/#operations-per-filter","title":"Operations per filter","text":"<p>Now the number of filters \\(\\psi\\) are known, it is necessary to compute how many operations each filter performs. As shown in Figure 1, for each kernel position there will be \\(\\text{kernel\\_size}\\) multiplications (i.e. each kernel element multiplied by a slice of the input tensor of the same size) and \\(\\text{kernel\\_size}-1\\) additions to aggregate the result and obtain one element of the output.</p> Figure 1. Operations per kernel position to obtain the output tensor. <p>Since each element in \\(\\left(H_\\text{out}, W_\\text{out}\\right)\\) is the result of the operations carried out for a single kernel position, the number of operations per filter \\(\\lambda\\) can be expressed as</p> \\[ \\begin{equation}     \\lambda=\\left(H_{\\text{out}}\\times W_{\\text{out}}\\right)\\times\\left(2\\times\\text{kernel\\_size[0]}\\times\\text{kernel\\_size[1]}-1\\right) \\end{equation} \\] <p>Because the kernel is a 2-dimensional tensor of dimensions \\(\\left(\\text{kernel\\_size[0]},\\text{kernel\\_size[1]}\\right).\\) </p> <p>Note</p> <p>Please note that the batch size \\(N\\) will be ignored for now, but it will be included later on.</p>"},{"location":"modules/conv2d/#filter-aggregation","title":"Filter aggregation","text":"<p>Now that the number of filters and the number of operations per filter are known, it is necessary compute the operations needed to aggregate each group of filters \\(\\gamma\\) to produce each output channel \\(C_\\text{out}\\). These operations correspond to simple element-wise additions and can be expressed as</p> \\[ \\begin{equation}     \\gamma=C_{\\text{out}}\\times H_\\text{out}\\times W_\\text{out}\\times\\left(\\left(\\frac{C_{\\text{in}}}{\\text{groups}}-1\\right)+1\\right) \\end{equation} \\] <p>Where the term \\(\\left(\\frac{C_{\\text{in}}}{\\text{groups}}-1\\right)\\) corresponds to the number of grouped connections between input and outputs channels \\(\\frac{C_{\\text{in}}}{\\text{groups}}\\), subtracted by \\(1\\) because the operation is an addition. The \\(H_\\text{out}\\times W_\\text{out}\\) factor accounts for the number of elements per filters, and \\(C_{\\text{out}}\\) expands this calculation to all output channels. Finally, the remaining \\(+1\\) corresponds to the bias term \\(b\\) that was not included so far, and that is added to each resulting output channel element. Note that this last term is only added if the module is instantiated using <code>bias=True</code>.</p> \\[ \\begin{equation} \\gamma=\\begin{cases}     C_{\\text{out}}\\times H_\\text{out}\\times W_\\text{out}\\times\\left(\\frac{C_{\\text{in}}}{\\text{groups}}\\right), &amp; \\text{if}\\ \\text{bias}=\\text{True} \\\\     C_{\\text{out}}\\times H_\\text{out}\\times W_\\text{out}\\times\\left(\\frac{C_{\\text{in}}}{\\text{groups}}-1\\right), &amp; \\text{if}\\ \\text{bias}=\\text{False} \\end{cases} \\end{equation} \\] <p>Note</p> <p>Please note that the bias term \\(b\\) was not included in  Operations per filter and is added here instead. Even though according to PyTorch <code>torch.nn.Conv2d</code> documentation \\(b\\) has shape \\(\\left(C_\\text{out}\\right)\\), in practice this tensor is implicitly broadcasted following PyTorch broadcasting semantics in such a way that each tensor value will be added with its corresponding channel bias.</p>"},{"location":"modules/conv2d/#total-operations","title":"Total operations","text":"<p>Now putting together all different factors that contribute to the total number of operations as well as including the batch size \\(N\\)</p> \\[ \\begin{equation}     \\text{Conv2d}_{ops}=N\\times\\left(\\psi\\times\\lambda+\\gamma\\right) \\end{equation} \\] <p>Where</p> <ul> <li>\\(N\\) is the batch size.</li> <li>\\(\\psi\\) is the number of filters.</li> <li>\\(\\lambda\\) is the number of operations per filter.</li> <li>\\(\\gamma\\) is the number of filter aggregation operations.</li> </ul> <p>For the case of <code>bias=True</code> this can be expanded to</p> \\[ \\begin{equation}     \\scriptsize{\\text{Conv2d}_{ops}=N\\times\\left(\\left(\\frac{C_{\\text{in}}\\times C_{\\text{out}}}{\\text{groups}}\\right)\\times\\left(H_{\\text{out}}\\times W_\\text{out}\\times\\left(2\\times\\text{kernel\\_size[0]}\\times\\text{kernel\\_size[1]}-1\\right)\\right)+C_{\\text{out}}\\times H_\\text{out}\\times W_\\text{out}\\times\\left(\\frac{C_{\\text{in}}}{\\text{groups}}\\right)\\right)} \\end{equation} \\] <p>Rearranging terms it can be simplified to</p> \\[ \\begin{equation} \\text{Conv2d}_{ops}=N\\times\\left(\\frac{C_{\\text{in}}\\times C_{\\text{out}}\\times H_{\\text{out}}\\times W_\\text{out}\\times2\\times\\text{kernel\\_size[0]}\\times\\text{kernel\\_size[1]}}{\\text{groups}}\\right) \\end{equation} \\] <p>For the case of <code>bias=False</code> \\(\\gamma=C_{\\text{out}}\\times H_\\text{out}\\times W_\\text{out}\\times\\left(\\frac{C_{\\text{in}}}{\\text{groups}}-1\\right)\\)  and the whole expression can be simplified to</p> \\[ \\begin{equation} \\text{Conv2d}_{ops}=N\\times\\left(\\frac{C_{\\text{out}}\\times H_{\\text{out}}\\times W_\\text{out}\\times\\left(C_{\\text{in}}\\times 2\\times\\text{kernel\\_size[0]}\\times\\text{kernel\\_size[1]}-\\text{groups}\\right)}{\\text{groups}}\\right) \\end{equation} \\]"},{"location":"modules/conv2d/#summary","title":"Summary","text":"<p>The number of operations performed by a <code>torch.nn.Conv2d</code> module can be estimated as</p> If <code>bias=True</code>If <code>bias=False</code> <p>\\(\\large{\\text{Conv2d}_{ops}=N\\times\\left(\\frac{C_{\\text{in}}\\times C_{\\text{out}}\\times H_{\\text{out}}\\times W_\\text{out}\\times2\\times\\text{kernel\\_size[0]}\\times\\text{kernel\\_size[1]}}{\\text{groups}}\\right)}\\)</p> <p>\\(\\large{\\text{Conv2d}_{ops}=N\\times\\left(\\frac{C_{\\text{out}}\\times H_{\\text{out}}\\times W_\\text{out}\\times\\left(C_{\\text{in}}\\times 2\\times\\text{kernel\\_size[0]}\\times\\text{kernel\\_size[1]}-\\text{groups}\\right)}{\\text{groups}}\\right)}\\)</p> <p>Where</p> <ul> <li>\\(N\\) is the batch size.</li> <li>\\(C_{\\text{in}}\\) is the number of input channels.</li> <li>\\(C_{\\text{out}}\\) is the number of output channels.</li> <li>\\(H_{\\text{in}}\\) is the height of the input tensor (i.e. <code>x.size(-2)</code> assuming an input tensor <code>x</code>)</li> <li>\\(W_{\\text{in}}\\) is the width of the input tensor (i.e. <code>x.size(-1)</code> assuming an input tensor <code>x</code>)</li> <li>\\(\\text{groups}\\) is the number of groups.</li> <li>\\(\\text{kernel\\_size[0]}\\) and \\(\\text{kernel\\_size[1]}\\) are the first and second dimensions of the <code>kernel_size</code> tuple.</li> </ul>"},{"location":"modules/convtranspose1d/","title":"ConvTranspose1d (<code>torch.nn.ConvTranspose1d</code>)","text":"<p>A <code>torch.nn.ConvTranspose1d</code> modules applies a transposed convolution along a given dimension of a tensor. This operation can be seen as the gradient of a <code>torch.nn.Conv1d</code> convolution with respect to its input. Is it also known as deconvolution, however, this might be misleading because a deconvolution is the inverse of a convolution operation.</p> <p>A <code>torch.nn.ConvTranspose1d</code> module expects an input of size \\(\\left(N,C_{in}, L_{in}\\right)\\) or \\(\\left(C_{in}, L_{in}\\right)\\) to produce an output of size \\(\\left(N,C_{out}, L_{out}\\right)\\) or \\(\\left(C_{out}, L_{out}\\right)\\). The relationship between layer parameters and \\(L_{out}\\) is defined as</p> \\[ \\begin{equation}     L_{out}=\\left(L_{in}-1\\right)\\times \\text{stride}-2\\times\\text{padding}+\\text{dilation}\\times\\left(\\text{kernel\\_size}-1\\right)+\\text{output\\_padding}+1 \\end{equation} \\] <p>Where</p> <ul> <li>\\(N\\) is the batch size.</li> <li>\\(C_{in}\\) is the number of input channels.</li> <li>\\(C_{out}\\) is the number of output channels.</li> <li>\\(L_{in}\\) is the length of the input tensor (i.e. <code>x.size(-1)</code> assuming an input tensor <code>x</code>).</li> <li>\\(L_{out}\\) is the length of the output tensor (i.e. <code>y.size(-1)</code> assuming an output tensor <code>y</code>).</li> </ul> <p>The remaining parameters are assumed to be known by the reader and can be found in the torch.nn.ConvTranspose1d documentation.</p>"},{"location":"modules/convtranspose1d/#complexity","title":"Complexity","text":""},{"location":"modules/convtranspose1d/#number-of-filters","title":"Number of filters","text":"<p>In order to calculate the number of operations performed this module, it is necessary to understand the impact of the <code>groups</code> parameter on the overall complexity, and the number of filters \\(\\psi\\) a network instance will have based on this. According to the official <code>torch.nn.ConvTranspose1d</code> documentation</p> <p><code>groups</code>\u00a0controls the connections between inputs and outputs.\u00a0<code>in_channels</code>\u00a0and\u00a0<code>out_channels</code>\u00a0must both be divisible by\u00a0<code>groups</code>.</p> <p>For example: At <code>groups=1</code>, all inputs are convolved to all outputs. At <code>groups=2</code>, the operation becomes equivalent to having two conv(transpose1d) layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.</p> <p>At <code>groups=in_channels</code>, each input channel is convolved with its own set of filters (of size \\(\\frac{\\text{out\\_channels}}{\\text{in\\_channels}}\\)\u00a0) </p> <p>Based on this information, the number of filters \\(\\psi\\) can be computed as</p> \\[ \\begin{equation}     \\psi = \\left(\\frac{C_{in}}{\\text{groups}}\\right)\\times\\left(\\frac{C_{out}}{\\text{groups}}\\right)\\times{\\text{groups}}=\\frac{C_{in}\\times C_{out}}{\\text{groups}} \\end{equation} \\]"},{"location":"modules/convtranspose1d/#operations-per-filter","title":"Operations per filter","text":"<p>Now the number of filters \\(\\psi\\) are known, it is necessary to compute how many operations each filter performs. As shown in Figure 1, for each kernel position there will be \\(\\text{kernel\\_size}\\) multiplications (i.e. each input element multiplied by the entire kernel). However, the additions pattern is more complicated compared to <code>torch.nn.Conv1d</code> because some kernel positions may overlap only partially. The possible outcomes become even more varied if the kernel has <code>dilation &gt; 1</code> or <code>stride &gt; 1</code>.</p> Figure 1. Operations per kernel position to obtain the output tensor. <p>For this case where obtaining a closed formula may be challenging, the approach to obtain the number of sums will be empirical. First, an input tensor \\(x\\) will be filled with ones. Then, a <code>nn.ConvTranspose1d</code> will be instantiated, with <code>bias=False</code> and its kernel will also be initialized filled with ones. By doing this, it is possible to observe from Figure 1 that the output sequence will be \\(y_0=1\\), \\(y_1=2\\), \\(y_2=2\\), and \\(y_3=1\\). By subtracting \\(1\\) to all values and adding the together, the result is the number of sums, \\(2\\) in this case. Please find a code snippet below to illustrate this.</p> <p>convtranspose1d_additions.py<pre><code>import torch\n\n# Input tensor\nx = torch.ones((1, 2))\n\n# Module\nconvtranspose1d = torch.nn.ConvTranspose1d(in_channels=1, out_channels=1, kernel_size=3, bias=False)\n\n# Fill weight with ones\ntorch.nn.init.ones_(convtranspose1d.weight)\n\n# Compute number of additions\nadditions = convtranspose1d(x) - 1.0  # tensor([[0., 1., 1., 0.]])\nadditions = torch.sum(additions)  #\u00a0tensor(2.)\n</code></pre> Each element in \\(L_\\text{out}\\) is the result of \\(\\text{kernel\\_size}\\) multiplications, and a number of additions that depends on possibly multiple overlapping kernel positions. The number of operations per filter \\(\\lambda\\) can be expressed as</p> \\[ \\begin{equation}     \\lambda=L_{out}\\times\\text{kernel\\_size} + \\text{additions\\_per\\_filter}  \\end{equation} \\] <p>Where \\(\\text{additions\\_per\\_filter}\\) corresponds to the result of the function to calculate the number of additions per filter.</p> <p>Note</p> <p>Please note that the batch size \\(N\\) will be ignored for now, but it will be included later on.</p>"},{"location":"modules/convtranspose1d/#filter-aggregation","title":"Filter aggregation","text":"<p>Now that the number of filters and the number of operations per filter are known, it is necessary compute the operations needed to aggregate each group of filters \\(\\gamma\\) to produce each output channel \\(C_{out}\\). These operations correspond to simple element-wise additions and can be expressed as</p> \\[ \\begin{equation}     \\gamma=C_{out}\\times L_\\text{out}\\times\\left(\\left(\\frac{C_{in}}{\\text{groups}}-1\\right)+1\\right) \\end{equation} \\] <p>Where the term \\(\\left(\\frac{C_{\\text{in}}}{\\text{groups}}-1\\right)\\) corresponds to the number of grouped connections between input and outputs channels \\(\\frac{C_{\\text{in}}}{\\text{groups}}\\), subtracted by \\(1\\) because the operation is an addition. The \\(L_\\text{out}\\) factor accounts for the number of elements per filters, and \\(C_{\\text{out}}\\) expands this calculation to all output channels. Finally, the remaining \\(+1\\) corresponds to the bias term \\(b\\) that was not included so far, and that is added to each resulting output channel element. Note that this last term is only added if the module is instantiated using <code>bias=True</code>.</p> \\[ \\begin{equation} \\gamma=\\begin{cases}     C_{out}\\times L_{out}\\times\\left(\\frac{C_{in}}{\\text{groups}}\\right), &amp; \\text{if}\\ \\text{bias}=\\text{True} \\\\     C_{out}\\times L_{out}\\times\\left(\\frac{C_{in}}{\\text{groups}}-1\\right), &amp; \\text{if}\\ \\text{bias}=\\text{False} \\end{cases} \\end{equation} \\] <p>Note</p> <p>Please note that the bias term \\(b\\) was not included in  Operations per filter and is added here instead. Even though according to PyTorch <code>torch.nn.ConvTranspose1d</code> documentation \\(b\\) has shape \\(\\left(C_{out}\\right)\\), in practice this tensor is implicitly broadcasted following PyTorch broadcasting semantics in such a way that each tensor value will be added with its corresponding channel bias.</p>"},{"location":"modules/convtranspose1d/#total-operations","title":"Total operations","text":"<p>Now putting together all different factors that contribute to the total number of operations as well as including the batch size \\(N\\)</p> \\[ \\begin{equation}     \\text{ConvTranspose1d}_{ops}=N\\times\\left(\\psi\\times\\lambda+\\gamma\\right) \\end{equation} \\] <p>Where</p> <ul> <li>\\(N\\) is the batch size.</li> <li>\\(\\psi\\) is the number of filters.</li> <li>\\(\\lambda\\) is the number of operations per filter.</li> <li>\\(\\gamma\\) is the number of filter aggregation operations.</li> </ul> <p>For the case of <code>bias=True</code> this can be expanded to</p> \\[ \\begin{equation}     \\small{\\text{ConvTranspose1d}_{ops}=N\\times\\frac{C_{in}\\times C_{out}}{\\text{groups}}\\times \\left(L_{out}\\times\\left(\\text{kernel\\_size}+1\\right)+\\text{additions\\_per\\_filter}\\right)} \\end{equation} \\] <p>For the case of <code>bias=False</code> \\(\\gamma=C_{out}\\times L_{out}\\times\\left(\\frac{C_{in}}{\\text{groups}}-1\\right)\\)  and the whole expression can be simplified to</p> \\[ \\begin{equation}     \\small{\\text{ConvTranspose1d}_{ops}=N\\times\\frac{C_{in}\\times C_{out}}{\\text{groups}}\\times \\left(L_{out}\\times\\left(\\text{kernel\\_size}+1\\right)+\\text{additions\\_per\\_filter}\\right) - N\\times C_{out}\\times L_{out}} \\end{equation} \\]"},{"location":"modules/convtranspose1d/#summary","title":"Summary","text":"<p>The number of operations performed by a <code>torch.nn.ConvTranspose1d</code> module can be estimated as</p> If <code>bias=True</code>If <code>bias=False</code> <p>\\(\\large{\\text{ConvTranspose1d}_{ops}=N\\times\\frac{C_{in}\\times C_{out}}{\\text{groups}}\\times \\left(L_{out}\\times\\left(\\text{kernel\\_size}+1\\right)+\\text{additions\\_per\\_filter}\\right)}\\)</p> <p>\\(\\large{\\text{ConvTranspose1d}_{ops}=N\\times\\frac{C_{in}\\times C_{out}}{\\text{groups}}\\times \\left(L_{out}\\times\\left(\\text{kernel\\_size}+1\\right)+\\text{additions\\_per\\_filter}\\right) - N\\times C_{out}\\times L_{out}}\\)</p> <p>Where</p> <ul> <li>\\(N\\) is the batch size.</li> <li>\\(C_{\\text{in}}\\) is the number of input channels.</li> <li>\\(C_{\\text{out}}\\) is the number of output channels.</li> <li>\\(L_{\\text{out}}\\) is the length of the output tensor (i.e. <code>y.size(-1)</code> assuming an output tensor <code>y</code>).</li> <li>\\(\\text{kernel\\_size}\\) is the length of the kernel.</li> <li>\\(\\text{groups}\\) is the number of groups.</li> <li>\\(\\text{additions\\_per\\_filter}\\) is the result of the function to calculate the number of addition operations per filter described in Operations per filter.</li> </ul>"},{"location":"modules/convtranspose2d/","title":"ConvTranspose2d (<code>torch.nn.ConvTranspose2d</code>)","text":"<p>A <code>torch.nn.ConvTranspose2d</code> modules applies a transposed convolution along a given pair of dimensions of a tensor. This operation can be seen as the gradient of a <code>torch.nn.Conv2d</code> convolution with respect to its input. Is it also known as deconvolution, however, this might be misleading because a deconvolution is the inverse of a convolution operation.</p> <p>A <code>torch.nn.ConvTranspose2d</code> module expects an input of size \\(\\left(N,C_{in}, H_{in}, W_{in}\\right)\\) or \\(\\left(C_{in}, H_{in}, W_{in}\\right)\\) to produce an output of size \\(\\left(N,C_{out}, H_{out}, W_{out}\\right)\\) or \\(\\left(C_{out}, H_{out}, W_{out}\\right)\\). The relationship between layer parameters, \\(H_{out}\\) and \\(W_{out}\\) is defined as</p> \\[ \\begin{equation}     \\small{         H_{out}=\\left(H_{in}-1\\right)\\times \\text{stride[0]}-2\\times\\text{padding[0]}+\\text{dilation[0]}\\times\\left(\\text{kernel\\_size[0]}-1\\right)+\\text{output\\_padding[0]}+1     } \\end{equation} \\] \\[ \\begin{equation}     \\small{         W_{out}=\\left(W_{in}-1\\right)\\times \\text{stride[1]}-2\\times\\text{padding[1]}+\\text{dilation[1]}\\times\\left(\\text{kernel\\_size[1]}-1\\right)+\\text{output\\_padding[1]}+1     } \\end{equation} \\] <p>Where</p> <ul> <li>\\(N\\) is the batch size.</li> <li>\\(C_{in}\\) is the number of input channels.</li> <li>\\(C_{out}\\) is the number of output channels.</li> <li>\\(H_{in}\\) is the height of the input tensor (i.e. <code>x.size(-2)</code> assuming an input tensor <code>x</code>)</li> <li>\\(W_{in}\\) is the width of the input tensor (i.e. <code>x.size(-1)</code> assuming an input tensor <code>x</code>)</li> <li>\\(H_{out}\\) is the height of the output tensor (i.e. <code>y.size(-2)</code> assuming an output tensor <code>y</code>)</li> <li>\\(W_{out}\\) is the width of the output tensor (i.e. <code>y.size(-1)</code> assuming an output tensor <code>y</code>)</li> </ul> <p>The remaining parameters are assumed to be known by the reader and can be found in the torch.nn.ConvTranspose2d documentation.</p>"},{"location":"modules/convtranspose2d/#complexity","title":"Complexity","text":""},{"location":"modules/convtranspose2d/#number-of-filters","title":"Number of filters","text":"<p>In order to calculate the number of operations performed this module, it is necessary to understand the impact of the <code>groups</code> parameter on the overall complexity, and the number of filters \\(\\psi\\) a network instance will have based on this. According to the official <code>torch.nn.ConvTranspose2d</code> documentation</p> <p><code>groups</code>\u00a0controls the connections between inputs and outputs.\u00a0<code>in_channels</code>\u00a0and\u00a0<code>out_channels</code>\u00a0must both be divisible by\u00a0<code>groups</code>.</p> <p>For example: At <code>groups=1</code>, all inputs are convolved to all outputs. At <code>groups=2</code>, the operation becomes equivalent to having two conv(transpose1d) layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.</p> <p>At <code>groups=in_channels</code>, each input channel is convolved with its own set of filters (of size \\(\\frac{\\text{out\\_channels}}{\\text{in\\_channels}}\\)\u00a0) </p> <p>Based on this information, the number of filters \\(\\psi\\) can be computed as</p> \\[ \\begin{equation}     \\psi = \\left(\\frac{C_{in}}{\\text{groups}}\\right)\\times\\left(\\frac{C_{out}}{\\text{groups}}\\right)\\times{\\text{groups}}=\\frac{C_{in}\\times C_{out}}{\\text{groups}} \\end{equation} \\]"},{"location":"modules/convtranspose2d/#operations-per-filter","title":"Operations per filter","text":"<p>Now the number of filters \\(\\psi\\) are known, it is necessary to compute how many operations each filter performs. For each kernel position there will be \\(\\text{kernel\\_size[0]}\\times\\text{kernel\\_size[1]}\\) multiplications (i.e. each input element multiplied by the entire kernel). However, the additions pattern is more complicated compared to <code>torch.nn.Conv2d</code> because some kernel positions may overlap only partially. The possible outcomes become even more varied if the kernel has <code>dilation &gt; 1</code> or <code>stride &gt; 1</code>. Please see these animations to visually understand how these patterns occur. It is the 2-dimensional generalization of the patterns already shown in <code>torch.nn.ConvTranspose1d</code> complexity calculations.</p> <p>For this case where obtaining a closed formula may be challenging, the approach to obtain the number of sums will be empirical. First, an input tensor \\(x\\) will be filled with ones. Then, a <code>nn.ConvTranspose2d</code> will be instantiated, with <code>bias=False</code> and its kernel will also be initialized filled with ones. By doing this, it is possible to observe that similarly to the case of <code>nn.ConvTranspose1d</code>, we can obtain the additions pattern by subtracting \\(1\\) to all values, and adding the together. Please find a code snippet below to illustrate this.</p> convtranspose2d_additions.py<pre><code>import torch\n\n# Input tensor\nx = torch.ones((1, 2, 2))\n\n# Module\nconvtranspose2d = torch.nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=2, bias=False)\n\n# Fill weight with ones\ntorch.nn.init.ones_(convtranspose2d.weight)\n\n# Compute number of additions\nadditions = convtranspose2d(x) - 1.0  # tensor([[[0., 1., 0.], [1., 3., 1.], [0., 1., 0.]]])\nadditions = torch.sum(additions)  #\u00a0tensor(7.)\n</code></pre> <p>Each element in \\(H_{out}\\times W_{out}\\) is the result of \\(\\text{kernel\\_size[0]}\\times\\text{kernel\\_size[1]}\\) multiplications, and a number of additions that depends on possibly multiple overlapping kernel positions. The number of operations per filter \\(\\lambda\\) can be expressed as</p> \\[ \\begin{equation}     \\lambda=H_{out}\\times W_{out}\\times\\text{kernel\\_size[0]}\\times\\text{kernel\\_size[1]} + \\text{additions\\_per\\_filter}  \\end{equation} \\] <p>Where \\(\\text{additions\\_per\\_filter}\\) corresponds to the result of the function to calculate the number of additions per filter.</p> <p>Note</p> <p>Please note that the batch size \\(N\\) will be ignored for now, but it will be included later on.</p>"},{"location":"modules/convtranspose2d/#filter-aggregation","title":"Filter aggregation","text":"<p>Now that the number of filters and the number of operations per filter are known, it is necessary compute the operations needed to aggregate each group of filters \\(\\gamma\\) to produce each output channel \\(C_\\text{out}\\). These operations correspond to simple element-wise additions and can be expressed as</p> \\[ \\begin{equation}     \\gamma=C_{\\text{out}}\\times H_\\text{out}\\times W_\\text{out}\\times\\left(\\left(\\frac{C_{\\text{in}}}{\\text{groups}}-1\\right)+1\\right) \\end{equation} \\] <p>Where the term \\(\\left(\\frac{C_{\\text{in}}}{\\text{groups}}-1\\right)\\) corresponds to the number of grouped connections between input and outputs channels \\(\\frac{C_{\\text{in}}}{\\text{groups}}\\), subtracted by \\(1\\) because the operation is an addition. The \\(H_\\text{out}\\times W_\\text{out}\\) factor accounts for the number of elements per filters, and \\(C_{\\text{out}}\\) expands this calculation to all output channels. Finally, the remaining \\(+1\\) corresponds to the bias term \\(b\\) that was not included so far, and that is added to each resulting output channel element. Note that this last term is only added if the module is instantiated using <code>bias=True</code>.</p> \\[ \\begin{equation} \\gamma=\\begin{cases}     C_{\\text{out}}\\times H_\\text{out}\\times W_\\text{out}\\times\\left(\\frac{C_{\\text{in}}}{\\text{groups}}\\right), &amp; \\text{if}\\ \\text{bias}=\\text{True} \\\\     C_{\\text{out}}\\times H_\\text{out}\\times W_\\text{out}\\times\\left(\\frac{C_{\\text{in}}}{\\text{groups}}-1\\right), &amp; \\text{if}\\ \\text{bias}=\\text{False} \\end{cases} \\end{equation} \\] <p>Note</p> <p>Please note that the bias term \\(b\\) was not included in  Operations per filter and is added here instead. Even though according to PyTorch <code>torch.nn.ConvTranspose2d</code> documentation \\(b\\) has shape \\(\\left(C_\\text{out}\\right)\\), in practice this tensor is implicitly broadcasted following PyTorch broadcasting semantics in such a way that each tensor value will be added with its corresponding channel bias.</p>"},{"location":"modules/convtranspose2d/#total-operations","title":"Total operations","text":"<p>Now putting together all different factors that contribute to the total number of operations as well as including the batch size \\(N\\)</p> \\[ \\begin{equation}     \\text{ConvTranspose2d}_{ops}=N\\times\\left(\\psi\\times\\lambda+\\gamma\\right) \\end{equation} \\] <p>Where</p> <ul> <li>\\(N\\) is the batch size.</li> <li>\\(\\psi\\) is the number of filters.</li> <li>\\(\\lambda\\) is the number of operations per filter.</li> <li>\\(\\gamma\\) is the number of filter aggregation operations.</li> </ul> <p>For the case of <code>bias=True</code> this can be expanded to</p> \\[ \\begin{equation}     \\scriptsize{         \\text{ConvTranspose2d}_{ops} = N \\times \\frac{C_{in} \\times C_{out}}{\\text{groups}} \\times \\left( H_{out} \\times W_{out} \\times \\left( \\text{kernel\\_size}[0] \\times \\text{kernel\\_size}[1] + 1 \\right) + \\text{additions\\_per\\_filter} \\right)     } \\end{equation} \\] <p>For the case of <code>bias=False</code> \\(\\gamma=C_{out}\\times H_{out}\\times W_{out}\\times\\left(\\frac{C_{in}}{\\text{groups}}-1\\right)\\)  and the whole expression can be simplified to</p> \\[ \\begin{equation}     \\scriptsize{         \\text{ConvTranspose2d}_{ops} = N \\times \\frac{C_{in} \\times C_{out}}{\\text{groups}} \\times \\left( H_{out} \\times W_{out} \\times \\left( \\text{kernel\\_size}[0] \\times \\text{kernel\\_size}[1] + 1 \\right) + \\text{additions\\_per\\_filter} \\right) -  N\\times C_{out}\\times H_{out}\\times W_{out}     } \\end{equation} \\]"},{"location":"modules/convtranspose2d/#summary","title":"Summary","text":"<p>The number of operations performed by a <code>torch.nn.ConvTranspose2d</code> module can be estimated as</p> If <code>bias=True</code>If <code>bias=False</code> <p>\\(\\small{\\text{ConvTranspose2d}_{ops} = N \\times \\frac{C_{in} \\times C_{out}}{\\text{groups}} \\times \\left( H_{out} \\times W_{out} \\times \\left( \\text{kernel\\_size}[0] \\times \\text{kernel\\_size}[1] + 1 \\right) + \\text{additions\\_per\\_filter} \\right)}\\)</p> <p>\\(\\small{\\text{ConvTranspose2d}_{ops} = N \\times \\frac{C_{in} \\times C_{out}}{\\text{groups}} \\times \\left( H_{out} \\times W_{out} \\times \\left( \\text{kernel\\_size}[0] \\times \\text{kernel\\_size}[1] + 1 \\right) + \\text{additions\\_per\\_filter} \\right) -  N\\times C_{out}\\times H_{out}\\times W_{out}}\\)</p> <p>Where</p> <ul> <li>\\(N\\) is the batch size.</li> <li>\\(C_{in}\\) is the number of input channels.</li> <li>\\(C_{out}\\) is the number of output channels.</li> <li>\\(H_{out}\\) is the height of the output tensor (i.e. <code>y.size(-2)</code> assuming an output tensor <code>x</code>)</li> <li>\\(W_{out}\\) is the width of the output tensor (i.e. <code>y.size(-1)</code> assuming an output tensor <code>x</code>)</li> <li>\\(\\text{kernel\\_size[0]}\\) and \\(\\text{kernel\\_size[1]}\\) are the first and second dimensions of the <code>kernel_size</code> tuple.</li> <li>\\(\\text{groups}\\) is the number of groups.</li> <li>\\(\\text{additions\\_per\\_filter}\\) is the result of the function to calculate the number of addition operations per filter described in Operations per filter.</li> </ul>"},{"location":"modules/gru/","title":"GRU (<code>torch.nn.GRU</code>)","text":"<p>A <code>torch.nn.GRU</code> corresponds to a Gated Recurrent Unit. That is - in essence - an arrangement or <code>torch.nn.GRUCell</code> that can process an input tensor containing a sequence of step and can use cell arrangements of configurable depth. The equations that rule a <code>torch.nn.GRU</code> are the same as <code>torch.nn.GRUCell</code>, except for the sequence length and the number of layers. Differently from a single <code>torch.nn.GRUCell</code>, a <code>torch.nn.GRU</code> has a hidden state per sequence step (\\(h_t\\)), a reset gate per time step (\\(r_t\\)) and an update gate per time step (\\(z_t\\)), thus, there is also a \\(n\\) tensor per time step (\\(n_t\\)). Please note that the current step hidden state \\(h_t\\) depends on the previous step hidden state \\(h_{t-1}\\).</p> \\[ \\begin{align}     r_t &amp;= \\sigma\\left(W_{ir}x_t+b_{ir}+W_{hr}h_{\\left(t-1\\right)}+b_{hr}\\right) \\\\     z_t &amp;= \\sigma\\left(W_{iz}x_t+b_{iz}+W_{hz}h_{\\left(t-1\\right)}+b_{hz}\\right) \\\\     n_t &amp;= \\text{tanh}\\left(W_{in}x_t+b_{in}+r_t\\odot\\left(W_{hn}h_{\\left(t-1\\right)}+b_{hn}\\right)\\right) \\\\     h_t &amp;= (1-z_t)\\odot n_t+z_t\\odot h_{\\left(t-1\\right)} \\end{align} \\] <p>Where</p> <ul> <li>\\(x\\) is the input tensor of size \\(\\left(L, H_{in}\\right)\\) or \\(\\left(L, N, H_{in}\\right)\\) when <code>batch_first=False</code>, or \\(\\left(N, L, H_{in}\\right)\\) when <code>batch_first=True</code>.</li> <li>\\(h_t\\) is the hidden state tensor at sequence step \\(t\\) of size \\(\\left(N, H_{out}\\right)\\) or \\(\\left(H_{out}\\right)\\).</li> <li>\\(H_{in}\\) and \\(H_{out}\\) are the number of input and output features, respectively.</li> <li>\\(L\\) is the sequence length.</li> <li>\\(N\\) is the batch size.</li> <li>\\(W_{ir}\\), \\(W_{iz}\\) and \\(W_{in}\\) are weight tensors of size \\(\\left(H_{out}, H_{in}\\right)\\) in the first layer and \\(\\left(H_{out}, D\\times H_{out}\\right)\\) in subsequent layers.</li> <li>\\(W_{hr}\\), \\(W_{hz}\\) and \\(W_{hn}\\) are weight tensors of size \\(\\left(H_{out}, H_{out}\\right)\\) </li> <li>\\(D\\) is \\(2\\) if <code>bidirectional=True</code> and \\(1\\) if <code>bidirectional=False</code>.</li> <li>\\(\\sigma\\) is the sigmoid function and can be defined as \\(\\sigma\\left(x\\right)=\\frac{1}{1+e^{-x}}\\).</li> <li>\\(\\text{tanh}\\) is the hyperbolic tangent function and can be defined as \\(\\text{tanh}\\left(x\\right)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}\\).</li> <li>\\(\\odot\\) is the Hadamard product or element-wise product.</li> <li>\\(b_{ir}\\), \\(b_{iz}\\), \\(b_{in}\\), \\(b_{hr}\\), \\(b_{hz}\\) and \\(b_{hn}\\) are bias tensors of size \\(\\left(H_{out}\\right)\\).</li> </ul> <p>Note</p> <p>Please note that some weight tensor sizes may differ from Pytorch <code>torch.nn.GRU</code>'s documentation due to the fact that some tensors are stacked. For instance, \\(W_{ir}\\), \\(W_{iz}\\) and \\(W_{in}\\) tensors of each layer are implemented as a single tensor of size \\(\\left(3\\times H_{out}, H_{in}\\right)\\) for the first layer, and \\(\\left(3\\times H_{out}, D\\times H_{out}\\right)\\) for subsequent layers. Similarly \\(W_{hr}\\), \\(W_{hz}\\) and \\(W_{hn}\\) are implemented as a single tensor of size \\(\\left(3\\times H_{out}, H_{out}\\right)\\). The number of layers is controlled by the <code>num_layers</code> parameter, and the number of directions \\(D\\) is controlled by the <code>birectional</code> parameter.</p> <p>Note</p> <p>The complexity of the <code>dropout</code> parameter is not considered in the following calculations, since it is usually temporarily used during training and then disabled during inference.</p>"},{"location":"modules/gru/#complexity","title":"Complexity","text":"<p>It is possible to reuse the calculation for <code>torch.nn.GRUCell</code> to estimate the complexity of <code>torch.nn.GRU</code>. However, there are a couple of additional considerations. First, when <code>num_layers &gt; 1</code>, the second layer takes the output(s) of the first layer as input. This means that \\(W_{ir}\\), \\(W_{iz}\\) and \\(W_{in}\\) will have size \\(\\left(H_{out}, H_{out}\\right)\\) if <code>bidirectional=False</code> and size \\(\\left(H_{out}, 2\\times H_{out}\\right)\\) if <code>bidirectional=True</code>. Secondly and differently from <code>torch.nn.GRUCell</code>, <code>torch.nn.GRU</code> can process an input containing bigger sequence lenghts, therefore the same calculations estimated before will repeat \\(L\\) times where \\(L\\) sequence length.</p> <p>Warning</p> <p>Please review the <code>torch.nn.GRUCell</code> complexity documentation before continuing, as the subsequent sections will reference formulas from that layer without re-deriving them.</p>"},{"location":"modules/gru/#unidirectional","title":"Unidirectional","text":"<p>The complexity of the first layer is the same as as <code>torch.nn.GRUCell</code> that if <code>bias=True</code>can be simplified to</p> \\[ \\begin{align}     \\text{GRU}_{ops}|_{\\text{layer}=0} = 6\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+3.5\\right) \\end{align} \\] <p>and when <code>bias=False</code></p> \\[ \\begin{align}     \\text{GRU}_{ops}|_{\\text{layer}=0} = 6\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+2.5\\right) \\end{align} \\] <p>For subsequent layers it is necessary to replace \\(H_{in}\\) by \\(H_{out}\\), then when <code>bias=True</code></p> \\[ \\begin{align}     \\text{GRU}_{ops}|_{\\text{layer}\\geq 1} = 6\\times N \\times H_{out}\\times\\left(2\\times H_{out}+3.5\\right) \\end{align} \\] <p>and when <code>bias=False</code></p> \\[ \\begin{align}     \\text{GRU}_{ops}|_{\\text{layer}\\geq 1} = 6\\times N \\times H_{out}\\times\\left(2\\times H_{out}+2.5\\right) \\end{align} \\]"},{"location":"modules/gru/#total-complexity","title":"Total complexity","text":"<p>Now it is necessary to include the sequence length \\(L\\) in the input tensor \\(x\\) to obtain the total complexity, since the previous calculation will be repeatead \\(L\\) times. The total complexity for <code>bidirectional=False</code> is</p> \\[ \\begin{align}     \\text{GRU}_{ops} &amp;= L\\times \\left(\\text{GRU}_{ops}|_{\\text{layer}=0} + \\left(\\text{num\\_layers} - 1 \\right)\\times \\text{GRU}_{ops}|_{\\text{layer}\\geq 1}\\right) \\end{align} \\] <p>When <code>bias=True</code> this expression becomes</p> \\[ \\begin{align}     \\text{GRU}_{ops} &amp;= L\\times \\underbrace{6\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+3.5\\right)}_{\\text{GRU}_{ops}|_{\\text{layer}=0}} \\nonumber \\\\     &amp;\\quad + L\\times \\left(\\text{num\\_layers} - 1 \\right)\\times \\underbrace{\\left(6\\times N \\times H_{out}\\times\\left(2\\times H_{out}+3.5\\right)\\right)}_{\\text{GRU}_{ops}|_{\\text{layer}\\geq 1}} \\nonumber \\\\     \\text{GRU}_{ops} &amp;= 6\\times L \\times N \\times H_{out}\\times \\left(H_{in}+\\left(2\\times\\text{num\\_layers}-1\\right)\\times H_{out}+3.5\\times\\text{num\\_layers}\\right) \\end{align} \\] <p>and when <code>bias=False</code></p> \\[ \\begin{align}     \\text{GRU}_{ops} &amp;= L\\times \\underbrace{6\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+2.5\\right)}_{\\text{GRU}_{ops}|_{\\text{layer}=0}} \\nonumber \\\\     &amp;\\quad + L\\times \\left(\\text{num\\_layers} - 1 \\right)\\times \\underbrace{\\left(6\\times N \\times H_{out}\\times\\left(2\\times H_{out}+2.5\\right)\\right)}_{\\text{GRU}_{ops}|_{\\text{layer}\\geq 1}} \\nonumber \\\\     \\text{GRU}_{ops} &amp;= 6\\times L \\times N \\times H_{out}\\times \\left(H_{in}+\\left(2\\times\\text{num\\_layers}-1\\right)\\times H_{out}+2.5\\times\\text{num\\_layers}\\right) \\end{align} \\]"},{"location":"modules/gru/#bidirectional","title":"Bidirectional","text":"<p>For the case of <code>bidirectional=True</code> the same considerations explained at the beginning of this section should be taken into account. Additionally, each cell will approximately duplicate its calculations because one subset of the output is calculated using the forward direction of the input sequence \\(x\\), and the remaining one uses the reverse input sequence \\(x\\). Please note that each direction of the input sequence will have its own set of weights, even though this is not documented at the moment of writing this documentation. Finally, both outputs will be concatenated to produce a tensor of size \\(\\left(L, N, D\\times H_{out}\\right)\\) with \\(D=2\\) in this case. When <code>num_layers &gt; 1</code>, this is also the size of the input size for layers after the first one.</p> <p>The complexity of the first layer when <code>bidirectional=True</code> and <code>bias=True</code> is</p> \\[ \\begin{align}     \\text{GRU}_{ops}|_{\\text{layer}=0} = 12\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+3.5\\right) \\end{align} \\] <p>and when <code>bias=False</code></p> \\[ \\begin{align}     \\text{GRU}_{ops}|_{\\text{layer}=0} = 12\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+2.5\\right) \\end{align} \\] <p>For subsequent layers it is necessary to replace \\(H_{in}\\) by \\(2\\times H_{out}\\). Then when <code>bias=True</code></p> \\[ \\begin{align}     \\text{GRU}_{ops}|_{\\text{layer}\\geq 1} = 12\\times N \\times H_{out}\\times\\left(3\\times H_{out}+3.5\\right) \\end{align} \\] <p>and when <code>bias=False</code></p> \\[ \\begin{align}     \\text{GRU}_{ops}|_{\\text{layer}\\geq 1} = 12\\times N \\times H_{out}\\times\\left(3\\times H_{out}+2.5\\right) \\end{align} \\]"},{"location":"modules/gru/#total-complexity_1","title":"Total complexity","text":"<p>Now it is necessary to include the sequence length \\(L\\) in the input tensor \\(x\\) to obtain the total complexity, since the previous calculation will be repeatead \\(L\\) times. The total complexity for <code>bidirectional=True</code> is</p> \\[ \\begin{align}     \\text{GRU}_{ops} &amp;= L\\times \\left(\\text{GRU}_{ops}|_{\\text{layer}=0} + \\left(\\text{num\\_layers} - 1 \\right)\\times \\text{GRU}_{ops}|_{\\text{layer}\\geq 1}\\right) \\end{align} \\] <p>When <code>bias=True</code> this expression becomes</p> \\[ \\begin{align}     \\text{GRU}_{ops} &amp;= L\\times\\underbrace{12\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+3.5\\right)}_{\\text{GRU}_{ops}|_{\\text{layer}=0}} \\nonumber \\\\     &amp;\\quad + L\\times\\left(\\text{num\\_layers} - 1 \\right)\\times \\underbrace{\\left(12\\times N \\times H_{out}\\times\\left(3\\times H_{out}+3.5\\right)\\right)}_{\\text{GRU}_{ops}|_{\\text{layer}\\geq 1}} \\nonumber \\\\     \\text{GRU}_{ops} &amp;= 12\\times L\\times N \\times H_{out}\\times \\left(H_{in}+\\left(3\\times\\text{num\\_layers}-2\\right)\\times H_{out}+3.5\\times\\text{num\\_layers}\\right) \\end{align} \\] <p>and when <code>bias=False</code></p> \\[ \\begin{align}     \\text{GRU}_{ops} &amp;= L\\times\\underbrace{12\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+2.5\\right)}_{\\text{GRU}_{ops}|_{\\text{layer}=0}} \\nonumber \\\\     &amp;\\quad + L\\times\\left(\\text{num\\_layers} - 1 \\right)\\times \\underbrace{\\left(12\\times N \\times H_{out}\\times\\left(3\\times H_{out}+2.5\\right)\\right)}_{\\text{GRU}_{ops}|_{\\text{layer}\\geq 1}} \\nonumber \\\\     \\text{GRU}_{ops} &amp;= 12\\times L\\times N \\times H_{out}\\times \\left(H_{in}+\\left(3\\times\\text{num\\_layers}-2\\right)\\times H_{out}+2.5\\times\\text{num\\_layers}\\right) \\end{align} \\]"},{"location":"modules/gru/#summary","title":"Summary","text":"<p>The number of operations performed by a <code>torch.nn.GRU</code> module can be estimated as</p> If <code>bias=True</code> and <code>bidirectional=False</code>If <code>bias=False</code> and <code>bidirectional=False</code>If <code>bias=True</code> and <code>bidirectional=True</code>If <code>bias=False</code> and <code>bidirectional=True</code> <p>\\(\\text{GRU}_{ops} = 6\\times L\\times N \\times H_{out}\\times \\left(H_{in}+\\left(2\\times\\text{num\\_layers}-1\\right)\\times H_{out}+3.5\\times\\text{num\\_layers}\\right)\\)</p> <p>\\(\\text{GRU}_{ops} = 6\\times L\\times N \\times H_{out}\\times \\left(H_{in}+\\left(2\\times\\text{num\\_layers}-1\\right)\\times H_{out}+2.5\\times\\text{num\\_layers}\\right)\\)</p> <p>\\(\\text{GRU}_{ops} = 12\\times L\\times N \\times H_{out}\\times \\left(H_{in}+\\left(3\\times\\text{num\\_layers}-2\\right)\\times H_{out}+3.5\\times\\text{num\\_layers}\\right)\\)</p> <p>\\(\\text{GRU}_{ops} = 12\\times L\\times N \\times H_{out}\\times \\left(H_{in}+\\left(3\\times\\text{num\\_layers}-2\\right)\\times H_{out}+2.5\\times\\text{num\\_layers}\\right)\\)</p> <p>Where</p> <ul> <li>\\(N\\) is the batch size.</li> <li>\\(H_\\text{in}\\) is the number of input features.</li> <li>\\(H_\\text{out}\\) is the number of output features.</li> <li>\\(L\\) is the sequence length.</li> <li>\\(\\text{num\\_layers}\\) is the number of layers. When <code>num_layers &gt; 1</code>, the output of the first layer is fed into the second one.</li> </ul>"},{"location":"modules/grucell/","title":"GRUCell (<code>torch.nn.GRUCell</code>)","text":"<p>A <code>torch.nn.GRUCell</code> corresponds to a single cell of a Grated Recurrent Unit (<code>torch.nn.GRU</code>). A <code>torch.nn.GRUCell</code> takes an input \\(x\\), a hidden state \\(h\\). Internally, it has a reset gate \\(r\\) and an update gate \\(z\\) that help to propagate information between time steps. These are combined to generate \\(n\\), that is then used to create a new hidden state \\(h\\prime\\). The relationship between these tensors is defines as</p> \\[ \\begin{align}     r &amp;= \\sigma\\left(W_{ir}x+b_{ir}+W_{hr}h+b_{hr}\\right) \\\\     z &amp;= \\sigma\\left(W_{iz}x+b_{iz}+W_{hz}h+b_{hz}\\right) \\\\     n &amp;= \\text{tanh}\\left(W_{in}x+b_{in}+r\\odot\\left(W_{hn}h+b_{hn}\\right)\\right) \\\\     h' &amp;= (1-z)\\odot n+z\\odot h \\end{align} \\] <p>Where</p> <ul> <li>\\(x\\) is the input tensor of size \\(\\left(N, H_{in}\\right)\\) or \\(\\left(H_{in}\\right)\\).</li> <li>\\(h\\) is the hidden state tensor of size \\(\\left(N, H_{out}\\right)\\) or \\(\\left(H_{out}\\right)\\).</li> <li>\\(H_{in}\\) and \\(H_{out}\\) are the number of input and output features, respectively.</li> <li>\\(N\\) is the batch size.</li> <li>\\(W_{ir}\\), \\(W_{iz}\\) and \\(W_{in}\\) are weight tensors of size \\(\\left(H_{out}, H_{in}\\right)\\). </li> <li>\\(W_{hr}\\), \\(W_{hz}\\) and \\(W_{hn}\\) are weight tensors of size \\(\\left(H_{out}, H_{out}\\right)\\). </li> <li>\\(\\sigma\\) is the sigmoid function and can be defined as \\(\\sigma\\left(x\\right)=\\frac{1}{1+e^{-x}}\\).</li> <li>\\(\\text{tanh}\\) is the hyperbolic tangent function and can be defined as \\(\\text{tanh}\\left(x\\right)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}\\).</li> <li>\\(\\odot\\) is the Hadamard product or element-wise product.</li> <li>\\(b_{ir}\\), \\(b_{iz}\\), \\(b_{in}\\), \\(b_{hr}\\), \\(b_{hz}\\) and \\(b_{hn}\\) are bias tensors of size \\(\\left(H_{out}\\right)\\).</li> </ul>"},{"location":"modules/grucell/#complexity","title":"Complexity","text":"<p>In order to compute the complexity of a single <code>torch.nn.GRUCell</code>, it is necessary to estimate the number of operations of all four aforementioned equations. For the sake of simplicity, for operations involving sigmoid and hyperbolic tangent, the listed equations will be used and exponentials will be counted as a single operation.</p> <p>Note</p> <p>During the following operations, some tensors have to be transposed in order to have compatible dimensions to perform matrix multiplication, even thought this is not explicitly mentioned in PyTorch <code>torch.nn.GRUCel</code>\u2019s documentation. Additionally, some weight tensors are stacked. For instance, \\(W_{ir}\\), \\(W_{iz}\\) and \\(W_{in}\\) are implemented as a single tensor of size \\(\\left(3\\times H_{out}, H_{in}\\right)\\), and \\(W_{hr}\\), \\(W_{hz}\\) and \\(W_{hn}\\) are implemented as a single tensor of size \\(\\left(3\\times H_{out}, H_{out}\\right)\\), possibly due to efficiency reasons.</p>"},{"location":"modules/grucell/#reset-gate","title":"Reset gate","text":"<p>The tensor sizes involved in the operations performed to calculate the reset gate \\(r\\) are</p> \\[ \\begin{align}     r = \\sigma\\Bigg(\\underbrace{\\left(H_{out}, H_{in}\\right) \\times \\left(N, H_{in}\\right)}_{W_{ir}x} + \\underbrace{H_{out}}_{b_{ir}} + \\underbrace{\\left(H_{out}, H_{out}\\right) \\times \\left(N, H_{out}\\right)}_{W_{hr}h} + \\underbrace{H_{out}}_{b_{hr}}\\Bigg) \\end{align} \\] <p>In this case, \\(x\\) (with shape \\(\\left(N, H_{in}\\right)\\)) and \\(h\\) (with shape \\(\\left(N, H_{out}\\right)\\)) have to be transposed. Additionally, \\(b_{ir}\\) and \\(b_{hr}\\) will be implicitly broadcasted to be able to be summed with the tensor multiplication results. Then, the unwrapped and transposed shapes involved in the operations are</p> \\[ \\begin{align}     r = \\sigma\\left(\\left(H_{out}, H_{in}\\right) \\times \\left(H_{in}, N\\right) + \\left(H_{out}, N\\right) + \\left(H_{out}, H_{out}\\right) \\times \\left(H_{out}, N\\right) + \\left(H_{out}, N\\right)\\right) \\end{align} \\] <p>This will result in a tensor of shape \\(\\left(H_{out}, N\\right)\\). To estimate the complexity of this operation, it is possible to reuse the results from <code>torch.nn.Linear</code> for both matrix multiplications and add the sigmoid operations \\(\\sigma\\). \\(r_{ops}\\) (the operations of the reset gate \\(r\\)) can be then broken down into four parts:</p> <ol> <li>The operations needed to compute \\(W_{ir}x+b_{ir}\\).</li> <li>The operations needed to compute \\(W_{hi}h+b_{hi}\\).</li> <li>The operations needed to sum both results.</li> <li>The operations needed to compute the sigmoid function \\(\\sigma\\) of this result.</li> </ol> <p>For simplicity sake, the following definitions will be used:</p> \\[ \\begin{align}     r = \\sigma\\left(\\underbrace{W_{ir}x^T+b_{ir}}_{r_0}+\\underbrace{W_{hr}h^T+b_{hr}}_{r_1}\\right) \\end{align} \\] <p>Then, in terms of operations (\\(ops\\)) when <code>bias=True</code></p> \\[ \\begin{align}     r_{0_{ops}} &amp;=\\left(W_{ir}x^T+b_{ir}\\right)_{ops} = 2\\times N\\times H_{out}\\times H_{in} \\\\     r_{1_{ops}} &amp;=\\left(W_{hr}x^T+b_{hr}\\right)_{ops} = 2\\times N\\times H_{out}^2 \\\\     \\left(r_0+r_1\\right)_{ops} &amp;= N\\times H_{out} \\\\     \\sigma_{ops} &amp;= 3\\times N\\times H_{out} \\\\     r_{ops} &amp;= 2\\times N\\times H_{out}\\times\\left(2+H_{in}+ H_{out}\\right) \\end{align} \\] <p>and when <code>bias=False</code></p> \\[ \\begin{align}     r_{0_{ops}} &amp;=\\left(W_{ir}x^T\\right)_{ops} = N\\times H_{out}\\times \\left(2\\times H_{in}-1\\right) \\\\     r_{1_{ops}}     &amp;=\\left(W_{hr}x^T\\right)_{ops}=N\\times H_{out}\\times\\left(2\\times H_{out}-1\\right) \\\\     \\left(r_0+r_1\\right)_{ops} &amp;= N\\times H_{out} \\\\     \\sigma_{ops} &amp;= 3\\times N\\times H_{out}\\\\     r_{ops} &amp;= 2\\times N\\times H_{out}\\times\\left(1+H_{in}+ H_{out}\\right) \\end{align} \\]"},{"location":"modules/grucell/#update-gate","title":"Update gate","text":"<p>Since the dimensions of this gate are the same as the reset gate \\(r\\), it is trivial to observe that</p> \\[ \\begin{equation}     z_{ops}=r_{ops} \\end{equation} \\]"},{"location":"modules/grucell/#n","title":"\\(n\\)","text":"<p>\\(n\\) has a slightly different configuration. Besides the matrix multiplications, there is Hadamard product \\(\\odot\\) and an hyperbolic tangent \\(\\text{tanh}\\) function. The involved tensor sizes are</p> \\[ \\begin{align}     n = \\text{tanh}\\Bigg(\\underbrace{\\left(H_{out}, H_{in}\\right) \\times \\left(N, H_{in}\\right)}_{W_{in}x} + \\underbrace{H_{out}}_{b_{in}} + \\underbrace{\\left(H_{out}, N\\right)}_{r}\\odot\\left(\\underbrace{\\left(H_{out}, H_{out}\\right) \\times \\left(N, H_{out}\\right)}_{W_{hn}h} + \\underbrace{H_{out}}_{b_{hn}}\\right)\\Bigg) \\end{align} \\] <p>Again, it becomes necessary to broadcast and transpose some tensors to be able to perform all operations, resulting in</p> \\[ \\begin{align}     n = \\text{tanh}\\left(\\left(H_{out}, H_{in}\\right) \\times \\left(H_{in}, N\\right) + \\left(H_{out}, N\\right) + \\left(H_{out}, N\\right)\\odot\\left(\\left(H_{out}, H_{out}\\right) \\times \\left(H_{out}, N\\right) + \\left(H_{out}, N\\right)\\right)\\right) \\end{align} \\] <p>Now \\(n_{ops}\\) (the operations performed by \\(n\\)) can be divided into five parts:</p> <ol> <li>The operations needed to compute \\(W_{in}x+b_{in}\\).</li> <li>The operations needed to compute \\(W_{hn}h+b_{hn}\\).</li> <li>The operations needed to compute the Hadamard product between \\(r\\) and \\(W_{hn}h+b_{hn}\\).</li> <li>The operations needed to sum the terms that result into the \\(\\text{tanh}\\) function argument.</li> <li>The operations needed to compute the hyperbolic tangent \\(\\text{tanh}\\) function of this result.</li> </ol> <p>Then, the different parts that contribute to \\(n_{ops}\\) can be defined as</p> \\[ \\begin{align}     n = \\text{tanh}\\left(\\underbrace{W_{in}x^T+b_{in}}_{n_0}+r\\odot\\left(\\underbrace{W_{hn}h^T+b_{hn}}_{n_1}\\right)\\right) \\end{align} \\] <p>Then, when <code>bias=True</code></p> \\[ \\begin{align}     n_{0_{ops}} &amp;= \\left(W_{in}x^T+b_{in}\\right)_{ops} = 2\\times N\\times H_{out}\\times H_{in} \\\\     n_{1_{ops}} &amp;= \\left(W_{hn}x^T+b_{hn}\\right)_{ops} = 2\\times N\\times H_{out}^2 \\\\     \\left(r\\odot n_1\\right)_{ops} &amp;= N\\times H_{out} \\\\     \\left(n_0+r\\odot n_1\\right)_{ops} &amp;= N\\times H_{out} \\\\     \\text{tanh}_{ops} &amp;= 7\\times N\\times H_{out} \\\\      n_{ops} &amp;= N\\times H_{out}\\times\\left(9+2\\times\\left(H_{in}+H_{out}\\right)\\right) \\end{align} \\] <p>and when <code>bias=False</code></p> \\[ \\begin{align}     n_{0_{ops}} &amp;= \\left(W_{in}x^T\\right)_{ops} = N\\times H_{out}\\times \\left(2\\times H_{in}-1\\right) \\\\     n_{1_{ops}} &amp;= \\left(W_{hn}x^T\\right)_{ops} = N\\times H_{out}\\times \\left(2\\times H_{out}-1\\right) \\\\     \\left(r\\odot n_1\\right)_{ops} &amp;= N\\times H_{out} \\\\     \\left(n_0+r\\odot n_1\\right)_{ops} &amp;= N\\times H_{out} \\\\     \\text{tanh}_{ops} &amp;= 7\\times N\\times H_{out} \\\\      n_{ops} &amp;= N\\times H_{out}\\times\\left(9+2\\times\\left(H_{in}+H_{out}-1\\right)\\right)  \\end{align} \\] <p>Note</p> <p>Please note that there are many possible formulations for the amount of operations carried out by \\(\\text{tanh}\\). In this calculation, the formula mentioned at the beginning is what is being used. In such a case, there are 7 operations per element are: x4 exponentials, x1 sum, x1 difference and x1 division. Please also note that we are ignoring sign inversion operations, assuming these will usually have a negligible computational cost. </p>"},{"location":"modules/grucell/#hprime","title":"\\(h\\prime\\)","text":"<p>The operations computed to obtain \\(h\\prime\\) can be divided into four parts:</p> <ol> <li>The operations needed to subtract \\(\\left(1-z\\right)\\).</li> <li>The operations needed to compute the Hadamard product between \\(\\left(1-z\\right)\\) and \\(n\\).</li> <li>The operations needed to compute the Hadamard product between \\(z\\) and \\(h\\).</li> <li>The operations needed to sum both Hadamard product results.</li> </ol> <p>In this case, all operations are element wise operations, therefore it is trivial to see that every part will contribute with \\(N\\times H_{out}\\) operations, therefore</p> \\[ \\begin{align} h\\prime_{ops} &amp;= 4\\times N\\times H_{out} \\end{align} \\]"},{"location":"modules/grucell/#total-complexity","title":"Total complexity","text":"<p>Finally, the total complexity is the sum of all individual contributions</p> \\[ \\begin{align}     \\text{GRUCell}_{ops}=r_{ops}+z_{ops}+n_{ops}+h\\prime_{ops} \\end{align} \\] <p>In the case of <code>bias=True</code>, the total number of operations is</p> \\[ \\begin{align}     \\text{GRUCell}_{ops}&amp;= \\underbrace{4\\times N\\times H_{out}\\left(2+H_{in}+ H_{out}\\right)}_{r_{ops}+z_{ops}} \\nonumber \\\\     &amp;\\quad + \\underbrace{N\\times H_{out}\\left(9+2\\left(H_{in}+H_{out}\\right)\\right)}_{n_{ops}}     \\nonumber \\\\     &amp;\\quad + \\underbrace{4\\times N\\times H_{out}}_{h\\prime_{ops}}     \\nonumber \\\\     \\text{GRUCell}_{ops}&amp;= 6\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+3.5\\right) \\end{align} \\] <p>and for <code>bias=False</code></p> \\[ \\begin{align}     \\text{GRUCell}_{ops}&amp;= \\underbrace{4\\times N\\times H_{out}\\left(1+H_{in}+ H_{out}\\right)}_{r_{ops}+z_{ops}} \\nonumber \\\\     &amp;\\quad + \\underbrace{N\\times H_{out}\\left(9+2\\left(H_{in}+H_{out}-1\\right)\\right)}_{n_{ops}}     \\nonumber \\\\     &amp;\\quad + \\underbrace{4\\times N\\times H_{out}}_{h\\prime_{ops}}     \\nonumber \\\\     \\text{GRUCell}_{ops}&amp;= 6\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+2.5\\right) \\end{align} \\]"},{"location":"modules/grucell/#summary","title":"Summary","text":"<p>The number of operations \\(\\phi\\) performed by a <code>torch.nn.GRUCell</code> module can be estimated as</p> If <code>bias=True</code>If <code>bias=False</code> <p>\\(\\text{GRUCell}_{ops} = 6\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+3.5\\right)\\)</p> <p>\\(\\text{GRUCell}_{ops} = 6\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+2.5\\right)\\)</p> <p>Where</p> <ul> <li>\\(N\\) is the batch size.</li> <li>\\(H_\\text{in}\\) is the number of input features.</li> <li>\\(H_\\text{out}\\) is the number of output features.</li> </ul>"},{"location":"modules/layernorm/","title":"LayerNorm (<code>torch.nn.LayerNorm</code>)","text":"<p>A <code>torch.nn.LayerNorm</code> module computes the mean and standard deviation over the last \\(D\\) dimensions specified by the <code>normalized_shape</code> parameter. If <code>elementwise_affine=True</code>, then two learnable parameters \\(\\gamma\\) and \\(\\beta\\) apply also an element-wise affine transformation that can be described as</p> \\[ \\begin{equation}     y=\\frac{x-\\text{E}\\left[x\\right]}{\\sqrt{\\text{Var}\\left[x\\right]+\\epsilon}}\\times \\gamma + \\beta \\end{equation} \\] <p>Where</p> <ul> <li>\\(x\\) is the input of size \\(\\left(N, \\ast\\right)\\)</li> <li>\\(\\text{E}\\left[x\\right]\\) is the mean of \\(x\\) over the last \\(D\\) dimensions.</li> <li>\\(\\text{Var}\\left[x\\right]\\) is the variance of \\(x\\) over the last \\(D\\) dimensions.</li> <li>\\(\\epsilon\\) is the machine epsilon added to avoid dividing by zero.</li> <li>\\(\\gamma\\) and \\(\\beta\\) are learnable parameters that are present if <code>elementwise_affine=True</code>.</li> </ul> <p>Note</p> <p>The standard deviation is calculated using a biased estimator, which is equivalent to <code>torch.var(input, correction=0)</code>.</p>"},{"location":"modules/layernorm/#complexity","title":"Complexity","text":"<p>The complexity of a <code>torch.nn.LayerNorm</code> layer can be divided into two parts: The aggregated statistics calculation (i.e. mean and standard deviation) and the affine transformation applied by \\(\\gamma\\) and \\(\\beta\\) if <code>elementwise_affine=True</code>.</p>"},{"location":"modules/layernorm/#aggregated-statistics","title":"Aggregated statistics","text":"<p>The complexity of the mean corresponds to the sum of all elements in the last \\(D\\) dimensions of the input tensor \\(x\\) and the division of that number by the total number of elements. As an example, if <code>normalized_shape=(3, 5)</code> then there are 14 additions and 1 division. This also corresponds to the product of the dimensions involved in <code>normalized_shape</code>.</p> \\[ \\begin{equation}     \\left(\\text{E}\\left[x\\right]\\right)_{ops} = \\prod_{d=0}^{D-1}\\text{normalized\\_shape}[\\text{d}] \\end{equation} \\] <p>Once \\(\\text{E}\\left[x\\right]\\) is obtained, it can be reused to obtain the variance using <code>torch.var</code> that is defined as</p> \\[ \\begin{equation}     \\text{Var}\\left[x\\right] = \\frac{1}{\\text{max}\\left(0, N-\\delta N\\right)}\\sum_{i=0}^{N-1}\\left(x_i-\\text{E}\\left[x\\right]\\right) \\end{equation} \\] <p>Where \\(\\delta N\\) is the correction (0 in this case). This step involves an element-wise subtraction, \\(N-1\\) additions to compute the sum. Additionally, a subtraction, a \\(\\text{max}\\) operation and a division are necessary to resolve the fraction. Then</p> \\[ \\begin{equation}     \\left(\\text{Var}\\left[x\\right]\\right)_{ops} = 2+2\\times\\prod_{d=0}^{D-1}\\text{normalized\\_shape}[\\text{d}] \\end{equation} \\] <p>Now, there are 2 additional operations (an addition and a square root) to obtain \\(\\sqrt{\\text{Var}\\left[x\\right]+\\epsilon}\\), therefore</p> \\[ \\begin{equation}     \\left(\\sqrt{\\text{Var}\\left[x\\right]+\\epsilon}\\right)_{ops} = 4+2\\times\\prod_{d=0}^{D-1}\\text{normalized\\_shape}[\\text{d}] \\end{equation} \\] <p>Finally, to obtain the whole fraction there is an additional element-wise subtraction in the numerator, and an element-wise division to divide the numerator by the denominator, therefore</p> \\[ \\begin{equation}     \\left(\\frac{x-\\text{E}\\left[x\\right]}{\\sqrt{\\text{Var}\\left[x\\right]+\\epsilon}}\\right)_{ops} = 4+5\\times\\prod_{d=0}^{D-1}\\text{normalized\\_shape}[\\text{d}] \\end{equation} \\]"},{"location":"modules/layernorm/#elementwise-affine","title":"Elementwise affine","text":"<p>If <code>elementwise_affine=True</code>, there is an element-wise multiplication by \\(\\gamma\\). If <code>bias=True</code>, there is also an element-wise addition by \\(\\beta\\). Therefore the whole complexity of affine transformations is</p> \\[ \\begin{equation}     \\gamma_{ops} = \\prod_{d=0}^{D-1}\\text{normalized\\_shape}[\\text{d}] \\end{equation} \\] <p>when <code>bias=False</code>, and</p> \\[ \\begin{equation}     \\gamma_{ops}+\\beta_{ops} = 2\\times\\prod_{d=0}^{D-1}\\text{normalized\\_shape}[\\text{d}] \\end{equation} \\] <p>when <code>bias=True</code>.</p>"},{"location":"modules/layernorm/#batch-size","title":"Batch size","text":"<p>So far we have not included the batch size \\(N\\), which in this case could be defined as all other dimensions that are not \\(D\\). This means, those that are not included in <code>normalized_shape</code>.</p> <p>Note</p> <p>Please note that \\(N\\) here corresponds to all dimensions not included in <code>normalized_shape</code>, which is different from the definition ot \\(N\\) in <code>torch.var</code> which corresponds to the number of elements in the input tensor of that function.  </p> <p>The batch size \\(N\\) multiplies all previously calculated operations by a factor \\(\\eta\\) corresponding to the multiplication of the remaining dimensions. For example, if the input tensor has size <code>(2, 3, 5)</code> and <code>normalized_shape=(3, 5)</code>, then \\(\\eta\\) is \\(2\\).</p>"},{"location":"modules/layernorm/#total-complexity","title":"Total complexity","text":"<p>Including all previously calculated factor, the total complexity can be summarized as</p> \\[ \\begin{equation}     \\text{LayerNorm}_{ops} = \\eta\\left(4+5\\times\\prod_{d=0}^{D-1}\\text{normalized\\_shape}[\\text{d}]\\right) \\end{equation} \\] <p>if <code>elementwise_affine=False</code> or </p> \\[ \\begin{equation}     \\text{LayerNorm}_{ops} = \\eta\\left(4+6\\times\\prod_{d=0}^{D-1}\\text{normalized\\_shape}[\\text{d}]\\right) \\end{equation} \\] <p>if <code>elementwise_affine=True</code> and <code>bias=False</code>, and</p> \\[ \\begin{equation}     \\text{LayerNorm}_{ops} = \\eta\\left(4+7\\times\\prod_{d=0}^{D-1}\\text{normalized\\_shape}[\\text{d}]\\right) \\end{equation} \\] <p>if <code>elementwise_affine=True</code> and <code>bias=True</code></p>"},{"location":"modules/layernorm/#summary","title":"Summary","text":"<p>The number of operations performed by a <code>torch.nn.LayerNorm</code> module can be estimated as</p> If <code>elementwise_affine=False</code>If <code>elementwise_affine=True</code> and <code>bias=False</code>If <code>elementwise_affine=True</code> and <code>bias=True</code> <p>\\(\\text{LayerNorm}_{ops} = \\displaystyle\\eta\\left(4+5\\times\\prod_{d=0}^{D-1}\\text{normalized\\_shape}[\\text{d}]\\right)\\) </p> <p>\\(\\text{LayerNorm}_{ops} = \\displaystyle\\eta\\left(4+6\\times\\prod_{d=0}^{D-1}\\text{normalized\\_shape}[\\text{d}]\\right)\\)</p> <p>\\(\\text{LayerNorm}_{ops} = \\displaystyle\\eta\\left(4+7\\times\\prod_{d=0}^{D-1}\\text{normalized\\_shape}[\\text{d}]\\right)\\)</p> <p>Where</p> <ul> <li>\\(\\eta\\) is the multiplication of all dimensions that are not included in <code>normalized_shape</code>.</li> <li>\\(D\\) is number of the last dimensions included in <code>normalized_shape</code>.</li> </ul> <p>As an example, if the input tensor has size <code>(2, 3, 5)</code> and <code>normalized_shape=(3, 5)</code>, then \\(D=15\\) and \\(\\eta=2\\).</p>"},{"location":"modules/linear/","title":"Linear (<code>torch.nn.Linear</code>)","text":"<p>A linear layer computes the following operation in a forward pass:</p> \\[ \\begin{equation}y=xA^T+b\\end{equation} \\] <p>Where</p> <ul> <li>\\(x\\) is a rank-N tensor of size \\(\\left(\\ast, H_\\text{in}\\right)\\) with N \\(\\geq\\) 1</li> <li>\\(A\\) is a weight rank-2 tensor of size  \\(\\left(H_\\text{out}, H_\\text{in}\\right)\\).</li> <li>\\(b\\) is a bias rank-1 tensor of size \\(\\left(H_\\text{out}\\right)\\).</li> <li>\\(y\\) is the output rank-N tensor of size \\(\\left(\\ast, H_\\text{out}\\right)\\).</li> <li>\\(\\ast\\) means any number of dimensions.</li> <li>\\(H_\\text{in}\\) is the number of input features.</li> <li>\\(H_\\text{out}\\) is the number of output features.</li> </ul> <p>The weight tensor \\(A\\) will apply a linear transformation or mapping to the input tensor \\(x\\), whereas the bias tensor \\(b\\) can be though of as a DC offset, since it is a learnable term that will act as a constant that is added to the result of the tensor-tensor multiplication \\(xA^T\\). </p>"},{"location":"modules/linear/#complexity","title":"Complexity","text":"<p>A linear module involves two tensor-tensor operations: one multiplication and one addition. In order to simplify the calculations,  \\(x\\) will be assumed to have size \\(\\left(1, H_\\text{in}\\right)\\). After computing the results, they will be expanded for higher dimensions. If \\(A\\) is a rank-2 tensor of size \\(\\left(H_\\text{out}, H_\\text{in}\\right)\\), then \\(A^T\\) has size \\(\\left(H_\\text{in}, H_\\text{out}\\right)\\). Therefore</p> \\[ \\begin{equation} xA^T=\\begin{pmatrix} x_0 &amp; ...&amp;x_{H_\\text{in}-1} \\end{pmatrix}\\times\\begin{pmatrix} a_{0,0} &amp; \\cdots&amp;a_{0,H_\\text{out}-1}\\\\ \\vdots &amp;  \\ddots &amp; \\vdots \\\\ a_{H_\\text{in}-1,0}&amp; \\cdots &amp; a_{H_\\text{in}-1,H_\\text{out}-1} \\end{pmatrix} \\end{equation} \\] <p>A single element \\(y_n\\) of the output tensor corresponds to the dot product of the first (and only) row of tensor \\(x\\) and a column of \\(A^T\\). As an example, the first output element \\(y_0\\) will be computed as</p> \\[ \\begin{equation} y_0=\\sum\\limits_{n=0}^{H_\\text{in}-1}x_n a_{n, 0}=x_0 a_{0, 0}+x_1 a_{1, 0}+\\cdots+x_{H_\\text{in}-1} a_{H_\\text{in}-1,0} \\end{equation} \\] <p>This operation requires \\(H_\\text{in}\\) multiplications and \\(H_\\text{in} - 1\\) additions. Therefore, the total number of operations per output feature is \\(2 \\times H_\\text{in} - 1\\). This has to be repeated \\(H_\\text{out}\\) times. Then, the total number of operations \\(\\text{Linear}_{ops}\\) so far is</p> \\[ \\begin{equation} \\text{Linear}_{ops}=H_\\text{out}\\times\\left(2 \\times H_\\text{in} - 1\\right) \\end{equation} \\] <p>Next, it is necessary to add the bias tensor \\(b\\). This is rather straightforward, since the result of \\(xA^T\\) has shape \\(\\left(1, H_\\text{out}\\right)\\) and the bias tensor \\(b\\) has shape \\(\\left(H_\\text{out}\\right)\\). The addition of the bias term corresponds therefore to \\(H_\\text{out}\\) additions</p> \\[ \\begin{equation} \\text{Linear}_{ops}=H_\\text{out}\\times\\left(2 \\times H_\\text{in} - 1\\right) + H_\\text{out} = 2\\times H_\\text{in}\\times H_\\text{out} \\end{equation} \\] <p>Depending on whether module was instantiated using <code>bias=True</code> or <code>bias=False</code>, there are two possible outcomes</p> \\[ \\begin{equation} \\text{Linear}_{ops}=\\begin{cases}     2\\times H_\\text{in}\\times H_\\text{out}, &amp; \\text{if}\\ \\text{bias}=\\text{True} \\\\     H_\\text{out}\\times\\left(2 \\times H_\\text{in} - 1\\right), &amp;\\text{if}\\ \\text{bias}=\\text{False} \\end{cases} \\end{equation} \\] <p>Finally, it is necessary to add the batch size. Since \\(\\ast\\) is any set of dimensions of \\(x\\). Given a rank-N tensor \\(x\\) of size \\(\\left(d_0, d_1, \\cdots, d_{N-1}\\right)\\) it is possible to define the batch size \\(\\beta\\) as</p> \\[ \\begin{equation} \\beta=\\prod^{N - 2}d_n=d_0\\times d_1\\times\\cdots\\times d_{N-2} \\end{equation} \\] <p>Note</p> <p>Please note that <code>torch.nn.Linear</code> allows the batch size \\(\\beta\\) to be composed of a single dimension or many, so its definition slightly differs from the batch size definition of other type of modules. As an example, if the input tensor \\(x\\) has size \\(\\left(2, 3, 4\\right)\\) then the batch dimension is \\(6\\), and the number of input features \\(H_\\text{in}\\) is \\(4\\). This is because <code>torch.nn.Linear</code> considers only the very last dimension as input features.</p> <p>The previously calculated number of operations is then repeated \\(\\beta\\) times. Finally, the total number of operations per forward pass is</p> \\[ \\begin{equation} \\text{Linear}_{ops}=\\begin{cases}     2\\times\\beta\\times H_\\text{out}\\times H_\\text{in}, &amp; \\text{if}\\ \\text{bias}=\\text{True} \\\\     \\beta\\times H_\\text{out}\\times\\left(2 \\times H_\\text{in} - 1\\right), &amp;\\text{if}\\ \\text{bias}=\\text{False} \\end{cases} \\end{equation} \\]"},{"location":"modules/linear/#summary","title":"Summary","text":"<p>The number of operations performed by a <code>torch.nn.Linear</code> module can be estimated as</p> If <code>bias=True</code>If <code>bias=False</code> <p>\\(\\text{Linear}_{ops} = 2\\times\\beta\\times H_\\text{out}\\times H_\\text{in}\\)</p> <p>\\(\\text{Linear}_{ops} = \\beta\\times H_\\text{out}\\times\\left(2 \\times H_\\text{in} - 1\\right)\\)</p> <p>Where</p> <ul> <li>\\(H_\\text{in}\\) is the number of input features.</li> <li>\\(H_\\text{out}\\) is the number of output features.</li> <li>\\(\\beta\\) is the batch size. For the case of <code>torch.nn.Linear</code> and a rank-N input tensor \\(x\\) of size \\(\\left(d_0, d_1, \\cdots, d_{N-1}\\right)\\) it is defined as \\(d_0\\times d_1\\times\\cdots\\times d_{N-2}\\). </li> </ul>"},{"location":"modules/lstm/","title":"LSTM (<code>torch.nn.LSTM</code>)","text":"<p>A <code>torch.nn.LSTM</code> corresponds to a Long-short Term Memory module. That is - in essence - an arrangement or <code>torch.nn.LSTMCell</code> that can process an input tensor containing a sequence of steps and can use cell arrangements of configurable depth. The equations that rule a <code>torch.nn.LSTM</code> are the same as <code>torch.nn.LSTMCell</code>, except for the sequence length and the number of layers. Differently from a single <code>torch.nn.LSTMCell</code>, a <code>torch.nn.LSTM</code> has a hidden state per sequence step (\\(h_t\\)), an input gate per sequence step (\\(i_t\\)), a forget gate per sequence step (\\(f_t\\)), a cell gate per sequence step (\\(g_t\\)), and an output gate per sequence step (\\(o_t\\)), thus, there is also a \\(c\\) tensor per time step (\\(c_t\\)), and an \\(h\\) tensor per time step (\\(h_t\\)). Please note that the current step hidden state \\(h_t\\) depends on the previous step hidden state \\(h_{t-1}\\).</p> \\[ \\begin{align}     i_t &amp;= \\sigma\\left(W_{ii}x_t+b_{ii}+W_{hi}h_{\\left(t-1\\right)}+b_{hi}\\right) \\\\     f_t &amp;= \\sigma\\left(W_{if}x_t+b_{if}+W_{hf}h_{\\left(t-1\\right)}+b_{hf}\\right) \\\\     g_t &amp;= \\text{tanh}\\left(W_{ig}x_t+b_{ig}+W_{hg}h_{\\left(t-1\\right)}+b_{hg}\\right) \\\\     o_t &amp;= \\sigma\\left(W_{io}x_t+b_{io}+W_{ho}h_{\\left(t-1\\right)}+b_{ho}\\right) \\\\     c_t &amp;= f_t\\odot c_{\\left(t-1\\right)}+i_t\\odot g_t \\\\     h_t &amp;= o_t \\odot\\text{tanh}\\left(c_t\\right) \\end{align} \\] <p>Where</p> <ul> <li>\\(x\\) is the input tensor of size \\(\\left(L, H_{in}\\right)\\) or \\(\\left(L, N, H_{in}\\right)\\) when <code>batch_first=True</code>, or \\(\\left(N, L, H_{in}\\right)\\) when <code>batch_first=True</code>.</li> <li>\\(h_t\\) is the hidden state tensor at sequence step \\(t\\) of size \\(\\left(N, H_{out}\\right)\\) or \\(\\left(H_{out}\\right)\\).</li> <li>\\(H_{in}\\) and \\(H_{out}\\) are the number of input and output features, respectively.</li> <li>\\(L\\) is the sequence length.</li> <li>\\(N\\) is the batch size.</li> <li>\\(c\\) is the cell state tensor of size \\(\\left(N, H_{out}\\right)\\) or \\(\\left(H_{out}\\right)\\).</li> <li>\\(W_{ii}\\), \\(W_{if}\\), \\(W_{ig}\\) and \\(W_{io}\\) are weight tensors of size \\(\\left(H_{out}, H_{in}\\right)\\) in the first layer and \\(\\left(H_{out}, D\\times H_{out}\\right)\\) in subsequent layers. </li> <li>\\(W_{hi}\\), \\(W_{hf}\\), \\(W_{hg}\\) and \\(W_{ho}\\) are weight tensors of size \\(\\left(H_{out}, H_{out}\\right)\\).</li> <li>\\(\\sigma\\) is the sigmoid function and can be defined as \\(\\sigma\\left(x\\right)=\\frac{1}{1+e^{-x}}\\).</li> <li>\\(\\text{tanh}\\) is the hyperbolic tangent function and can be defined as \\(\\text{tanh}\\left(x\\right)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}\\).</li> <li>\\(\\odot\\) is the Hadamard product or element-wise product.</li> <li>\\(b_{ii}\\), \\(b_{hi}\\), \\(b_{if}\\), \\(b_{hf}\\), \\(b_{ig}\\), \\(b_{hg}\\), \\(b_{io}\\) and \\(b_{ho}\\) are bias tensors of size \\(\\left(H_{out}\\right)\\).</li> </ul> <p>Note</p> <p>Please note that some weight tensor sizes may differ from Pytorch <code>torch.nn.LSTM</code>'s documentation due to the fact that some tensors are stacked. For instance, \\(W_{ii}\\), \\(W_{if}\\), \\(W_{ig}\\) and \\(W_{ino\\) tensors of each layer are implemented as a single tensor of size \\(\\left(4\\times H_{out}, H_{in}\\right)\\) for the first layer, and \\(\\left(4\\times H_{out}, D\\times H_{out}\\right)\\) for subsequent layers. Similarly \\(W_{hi}\\), \\(W_{hf}\\), \\(W_{hg}\\) and \\(W_{ho}\\) are implemented as a single tensor of size \\(\\left(4\\times H_{out}, H_{out}\\right)\\). The number of layers is controlled by the <code>num_layers</code> parameter, and the number of directions \\(D\\) is controlled by the <code>birectional</code> parameter.</p> <p>Note</p> <p>The complexity of the <code>dropout</code> parameter is not considered in the following calculations, since it is usually temporarily used during training and then disabled during inference.</p>"},{"location":"modules/lstm/#complexity","title":"Complexity","text":"<p>It is possible to reuse the calculation for <code>torch.nn.GRUCell</code> to estimate the complexity of <code>torch.nn.GRU</code>. However, there are a couple of additional considerations. First, when <code>num_layers &gt; 1</code>, the second layer takes the output(s) of the first layer as input. This means that \\(W_{ii}\\), \\(W_{if}\\), \\(W_{ig}\\) and \\(W_{io}\\) will have size \\(\\left(H_{out}, H_{out}\\right)\\) if <code>bidirectional=False</code> and size \\(\\left(H_{out}, 2\\times H_{out}\\right)\\) if <code>bidirectional=True</code>. Secondly and differently from <code>torch.nn.LSTMCell</code>, <code>torch.nn.LSTM</code> can process an input containing bigger sequence lenghts, therefore the same calculations estimated before will repeat \\(L\\) times where \\(L\\) sequence length.</p> <p>Warning</p> <p>Please review the <code>torch.nn.LSTMCell</code> complexity documentation before continuing, as the subsequent sections will reference formulas from that layer without re-deriving them.</p>"},{"location":"modules/lstm/#unidirectional","title":"Unidirectional","text":"<p>The complexity of the first layer is the same as as <code>torch.nn.LSTMCell</code> that if <code>bias=True</code>can be simplified to</p> \\[ \\begin{align}     \\text{LSTM}_{ops}|_{\\text{layer}=0} = 8\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+3.875\\right) \\end{align} \\] <p>and when <code>bias=False</code></p> \\[ \\begin{align}     \\text{LSTM}_{ops}|_{\\text{layer}=0} = 8\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+2.875\\right) \\end{align} \\] <p>For subsequent layers it is necessary to replace \\(H_{in}\\) by \\(H_{out}\\), then when <code>bias=True</code></p> \\[ \\begin{align}     \\text{LSTM}_{ops}|_{\\text{layer}\\geq 1} = 8\\times N \\times H_{out}\\times\\left(2\\times H_{out}+3.875\\right) \\end{align} \\]"},{"location":"modules/lstm/#total-complexity","title":"Total complexity","text":"<p>Now it is necessary to include the sequence length \\(L\\) in the input tensor \\(x\\) to obtain the total complexity, since the previous calculation will be repeatead \\(L\\) times. The total complexity for <code>bidirectional=False</code> is</p> \\[ \\begin{align}     \\text{LSTM}_{ops} = L\\times\\left(\\text{LSTM}_{ops}|_{\\text{layer}=0} + \\left(\\text{num\\_layers} - 1\\right)\\times \\text{LSTM}_{ops}|_{\\text{layer}\\geq 1}\\right) \\end{align} \\] <p>When <code>bias=True</code> this expression becomes</p> \\[ \\begin{align}     \\text{LSTM}_{ops} &amp;= L\\times \\underbrace{8\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+3.875\\right)}_{\\text{LSTM}_{ops}|_{\\text{layer}=0}} \\nonumber \\\\     &amp;\\quad + L\\times \\left(\\text{num\\_layers} - 1 \\right)\\times \\underbrace{8\\times N \\times H_{out}\\times\\left(2\\times H_{out}+3.875\\right)}_{\\text{LSTM}_{ops}|_{\\text{layer}\\geq 1}} \\nonumber \\\\     \\text{LSTM}_{ops} &amp;= 8\\times L \\times N \\times H_{out}\\times \\left(H_{in}+\\left(2\\times\\text{num\\_layers}-1\\right)\\times H_{out}+3.875\\times\\text{num\\_layers}\\right) \\end{align} \\] <p>and when <code>bias=False</code></p> \\[ \\begin{align}     \\text{LSTM}_{ops} &amp;= L\\times \\underbrace{8\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+2.875\\right)}_{\\text{LSTM}_{ops}|_{\\text{layer}=0}} \\nonumber \\\\     &amp;\\quad + L\\times \\left(\\text{num\\_layers} - 1 \\right)\\times \\underbrace{8\\times N \\times H_{out}\\times\\left(2\\times H_{out}+2.875\\right)}_{\\text{LSTM}_{ops}|_{\\text{layer}\\geq 1}} \\nonumber \\\\     \\text{LSTM}_{ops} &amp;= 8\\times L \\times N \\times H_{out}\\times \\left(H_{in}+\\left(2\\times\\text{num\\_layers}-1\\right)\\times H_{out}+2.875\\times\\text{num\\_layers}\\right) \\end{align} \\]"},{"location":"modules/lstm/#bidirectional","title":"Bidirectional","text":"<p>For the case of <code>bidirectional=True</code> the same considerations explained at the beginning of this section should be taken into account. Additionally, each cell will approximately duplicate its calculations because one subset of the output is calculated using the forward direction of the input sequence \\(x\\), and the remaining one uses the reverse input sequence \\(x\\). Please note that each direction of the input sequence will have its own set of weights. Finally, both outputs will be concatenated to produce a tensor of size \\(\\left(L, N, D\\times H_{out}\\right)\\) with \\(D=2\\) in this case. When <code>num_layers &gt; 1</code>, this is also the size of the input size for layers after the first one.</p> <p>The complexity of the first layer when <code>bidirectional=True</code> and <code>bias=True</code> is</p> \\[ \\begin{align}     \\text{LSTM}_{ops}|_{\\text{layer}=0} = 16\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+3.875\\right) \\end{align} \\] <p>and when <code>bias=False</code></p> \\[ \\begin{align}     \\text{LSTM}_{ops}|_{\\text{layer}=0} = 16\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+2.875\\right) \\end{align} \\] <p>For subsequent layers it is necessary to replace \\(H_{in}\\) by \\(2\\times H_{out}\\). Then when <code>bias=True</code></p> \\[ \\begin{align}     \\text{LSTM}_{ops}|_{\\text{layer}\\geq 1} = 16\\times N \\times H_{out}\\times\\left(3\\times H_{out}+3.875\\right) \\end{align} \\] <p>and when <code>bias=False</code></p> \\[ \\begin{align}     \\text{LSTM}_{ops}|_{\\text{layer}\\geq 1} = 16\\times N \\times H_{out}\\times\\left(3\\times H_{out}+2.875\\right) \\end{align} \\]"},{"location":"modules/lstm/#total-complexity_1","title":"Total complexity","text":"<p>Now it is necessary to include the sequence length \\(L\\) in the input tensor \\(x\\) to obtain the total complexity, since the previous calculation will be repeatead \\(L\\) times. The total complexity for <code>bidirectional=True</code> is</p> \\[ \\begin{align}     \\text{LSTM}_{ops} = L\\times\\left(\\text{LSTM}_{ops}|_{\\text{layer}=0} + \\left(\\text{num\\_layers} - 1\\right)\\times \\text{LSTM}_{ops}|_{\\text{layer}\\geq 1}\\right) \\end{align} \\] <p>When <code>bias=True</code> this expression becomes</p> \\[ \\begin{align}     \\text{LSTM}_{ops} &amp;= L\\times\\underbrace{16\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+3.875\\right)}_{\\text{LSTM}_{ops}|_{\\text{layer}=0}} \\nonumber \\\\     &amp;\\quad + L\\times\\left(\\text{num\\_layers} - 1 \\right)\\times \\underbrace{\\left(16\\times N \\times H_{out}\\times\\left(3\\times H_{out}+3.875\\right)\\right)}_{\\text{GRU}_{ops}|_{\\text{layer}\\geq 1}} \\nonumber \\\\     \\text{LSTM}_{ops} &amp;= 16\\times L\\times N \\times H_{out}\\times \\left(H_{in}+\\left(3\\times\\text{num\\_layers}-2\\right)\\times H_{out}+3.875\\times\\text{num\\_layers}\\right) \\end{align} \\] <p>and when <code>bias=False</code></p> \\[ \\begin{align}     \\text{LSTM}_{ops} &amp;= L\\times\\underbrace{16\\times N \\times H_{out}\\times\\left(H_{in}+H_{out}+2.875\\right)}_{\\text{LSTM}_{ops}|_{\\text{layer}=0}} \\nonumber \\\\     &amp;\\quad + L\\times\\left(\\text{num\\_layers} - 1 \\right)\\times \\underbrace{\\left(16\\times N \\times H_{out}\\times\\left(3\\times H_{out}+2.875\\right)\\right)}_{\\text{GRU}_{ops}|_{\\text{layer}\\geq 1}} \\nonumber \\\\     \\text{LSTM}_{ops} &amp;= 16\\times L\\times N \\times H_{out}\\times \\left(H_{in}+\\left(3\\times\\text{num\\_layers}-2\\right)\\times H_{out}+2.875\\times\\text{num\\_layers}\\right) \\end{align} \\]"},{"location":"modules/lstm/#summary","title":"Summary","text":"<p>The number of operations performed by a <code>torch.nn.LSTM</code> module can be estimated as</p> If <code>bias=True</code> and <code>bidirectional=False</code>If <code>bias=False</code> and <code>bidirectional=False</code>If <code>bias=True</code> and <code>bidirectional=True</code>If <code>bias=False</code> and <code>bidirectional=True</code> <p>\\(\\text{LSTM}_{ops} = 8\\times L\\times N \\times H_{out}\\times \\left(H_{in}+\\left(2\\times\\text{num\\_layers}-1\\right)\\times H_{out}+3.875\\times\\text{num\\_layers}\\right)\\)</p> <p>\\(\\text{LSTM}_{ops} = 8\\times L\\times N \\times H_{out}\\times \\left(H_{in}+\\left(2\\times\\text{num\\_layers}-1\\right)\\times H_{out}+2.875\\times\\text{num\\_layers}\\right)\\)</p> <p>\\(\\text{LSTM}_{ops} = 16\\times L\\times N \\times H_{out}\\times \\left(H_{in}+\\left(3\\times\\text{num\\_layers}-2\\right)\\times H_{out}+3.875\\times\\text{num\\_layers}\\right)\\)</p> <p>\\(\\text{LSTM}_{ops} = 16\\times L\\times N \\times H_{out}\\times \\left(H_{in}+\\left(3\\times\\text{num\\_layers}-2\\right)\\times H_{out}+2.875\\times\\text{num\\_layers}\\right)\\)</p> <p>Where </p> <ul> <li>\\(L\\) is the sequence length.</li> <li>\\(N\\) is the batch size.</li> <li>\\(H_{in}\\) and \\(H_{out}\\) are the number of input and output features, respectively.</li> <li>\\(\\text{num\\_layers}\\) is the number of layers.</li> </ul>"},{"location":"modules/lstmcell/","title":"LSTMCell (<code>torch.nn.LSTMCell</code>)","text":"<p>A <code>torch.nn.LSTMCell</code> correspond to a single cell of a Long Short-Term Memory Layer (<code>torch.nn.LSTM</code>). A <code>torch.nn.LSTMCell</code> takes in an input \\(x\\), a hidden state \\(h\\) and a cell state \\(c\\) . Internally, it has an input gate \\(i\\), a forget gate \\(f\\), a cell gate \\(g\\) and an output gate \\(o\\) that help to propagate information between sequence steps. These are combined to generate the <code>torch.nn.LSTMCell</code> outputs. The relationship between these tensors is defined as</p> \\[ \\begin{align}     i &amp;= \\sigma\\left(W_{ii}x+b_{ii}+W_{hi}h+b_{hi}\\right) \\\\     f &amp;= \\sigma\\left(W_{if}x+b_{if}+W_{hf}h+b_{hf}\\right) \\\\     g &amp;= \\text{tanh}\\left(W_{ig}x+b_{ig}+W_{hg}h+b_{hg}\\right) \\\\     o &amp;= \\sigma\\left(W_{io}x+b_{io}+W_{ho}h+b_{ho}\\right) \\\\     c\\prime &amp;= f\\odot c+i\\odot g \\\\     h\\prime &amp;= o \\odot\\text{tanh}\\left(c\\prime\\right) \\end{align} \\] <p>Where</p> <ul> <li>\\(x\\) is the input tensor of size \\(\\left(N, H_{in}\\right)\\) or \\(\\left(H_{in}\\right)\\).</li> <li>\\(h\\) is the hidden state tensor of size \\(\\left(N, H_{out}\\right)\\) or \\(\\left(H_{out}\\right)\\).</li> <li>\\(c\\) is the cell state tensor of size \\(\\left(N, H_{out}\\right)\\) or \\(\\left(H_{out}\\right)\\).</li> <li>\\(W_{ii}\\), \\(W_{if}\\), \\(W_{ig}\\) and \\(W_{io}\\) are weight tensors of size \\(\\left(H_{out}, H_{in}\\right)\\). </li> <li>\\(W_{hi}\\), \\(W_{hf}\\), \\(W_{hg}\\) and \\(W_{ho}\\) are weight tensors of size \\(\\left(H_{out}, H_{out}\\right)\\).</li> <li>\\(\\sigma\\) is the sigmoid function and can be defined as \\(\\sigma\\left(x\\right)=\\frac{1}{1+e^{-x}}\\).</li> <li>\\(\\text{tanh}\\) is the hyperbolic tangent function and can be defined as \\(\\text{tanh}\\left(x\\right)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}\\).</li> <li>\\(\\odot\\) is the Hadamard product or element-wise product.</li> <li>\\(b_{ii}\\), \\(b_{hi}\\), \\(b_{if}\\), \\(b_{hf}\\), \\(b_{ig}\\), \\(b_{hg}\\), \\(b_{io}\\) and \\(b_{ho}\\) are bias tensors of size \\(\\left(H_{out}\\right)\\).</li> </ul>"},{"location":"modules/lstmcell/#complexity","title":"Complexity","text":"<p>In order to compute the complexity of a single <code>torch.nn.LSTMCell</code>, it is necessary to estimate the number of operations of all six aforementioned equations. For the sake of simplicity, for operations involving sigmoid and hyperbolic tangent, the aforementioned equations will be used and exponentials will be counted as a single operation.</p> <p>Note</p> <p>During the following operations, some tensors have to be transposed in order to have compatible dimensions to perform matrix multiplication, even thought this is not explicitly mentioned in PyTorch <code>torch.nn.LSTMCell\u2019s</code> documentation. Additionally, some weight tensors are stacked. For instance, \\(W_{ii}\\), \\(W_{if}\\) \\(W_{ig}\\) and \\(W_{io}\\) are implemented as a single tensor of size \\(\\left(4\\times H_{out},H_{in} \\right)\\), and \\(W_{hi}\\), \\(W_{hf}\\), \\(W_{hg}\\) and \\(W_{ho}\\) are implemented as a single tensor of size \\(\\left(4\\times H_{out},H_{out} \\right)\\), possibly due to efficiency reasons.</p>"},{"location":"modules/lstmcell/#input-gate","title":"Input gate","text":"<p>The tensor sizes involved in the operations performed to calculate the input gate \\(i\\) are</p> \\[ \\begin{align}     i = \\sigma\\Bigg(\\underbrace{\\left(H_{out}, H_{in}\\right) \\times \\left(N, H_{in}\\right)}_{W_{ii}x} + \\underbrace{H_{out}}_{b_{ii}} + \\underbrace{\\left(H_{out}, H_{out}\\right) \\times \\left(N, H_{out}\\right)}_{W_{hi}h} + \\underbrace{H_{out}}_{b_{hi}}\\Bigg) \\end{align} \\] <p>In this case, \\(x\\) (with shape \\(\\left(N, H_{in}\\right)\\)) and \\(h\\) (with shape \\(\\left(N, H_{out}\\right)\\)) have to be transposed. Additionally, \\(b_{ii}\\) and \\(b_{hi}\\) will be implicitly broadcasted to be able to be summed with the tensor multiplication results. Then, the unwrapped and transposed shapes involved in the operations are</p> \\[ \\begin{align} i &amp;= \\sigma\\left(\\left(H_{out}, H_{in}\\right)\\times\\left(H_{in}, N\\right)+\\left(H_{out}, N\\right)+\\left(H_{out}, H_{out}\\right)\\times\\left(H_{out}, N\\right)+\\left(H_{out}, N\\right)\\right) \\end{align} \\] <p>This will result in a tensor of shape \\(\\left(H_{out}, N\\right)\\). To estimate the complexity of this operation, it is possible to reuse the results from <code>torch.nn.Linear</code> for both matrix multiplications and add the sigmoid operations \\(\\sigma\\). \\(i_{ops}\\) (the operations of the input gate \\(i\\)) can be then broken down into four parts:</p> <ol> <li>The operations to needed compute \\(W_{ii}x+b_{ii}\\).</li> <li>The operations needed to compute \\(W_{hi}h+b_{hi}\\).</li> <li>The operations needed to sum both results.</li> <li>The operations needed to compute the sigmoid function \\(\\sigma\\) of this result.</li> </ol> <p>For simplicity sake, the following definitions will be used:</p> \\[ \\begin{align}     i &amp;= \\sigma\\left(\\underbrace{W_{ii}x^T+b_{ii}}_{i_0}+\\underbrace{W_{hi}h^T+b_{hi}}_{i_1}\\right) \\end{align} \\] <p>Then, in terms of operations (\\(ops\\)) when <code>bias=True</code></p> \\[ \\begin{align}     i_{0_{ops}} &amp;=\\left(W_{ii}x^T+b_{ii}\\right)_{ops} = 2\\times N\\times H_{out}\\times H_{in} \\\\     i_{1_{ops}} &amp;=\\left(W_{hi}x^T+b_{hi}\\right)_{ops} = 2\\times N\\times H_{out}^2 \\\\     \\left(i_0+i_1\\right)_{ops} &amp;= N\\times H_{out} \\\\     \\sigma_{ops} &amp;= 3\\times N\\times H_{out} \\\\     i_{ops} &amp;= 2\\times N\\times H_{out}\\times\\left(2+H_{in}+ H_{out}\\right) \\end{align} \\] <p>and when <code>bias=False</code></p> \\[ \\begin{align}     i_{0_{ops}} &amp;=\\left(W_{ii}x^T\\right)_{ops} = N\\times H_{out}\\times \\left(2\\times H_{in}-1\\right) \\\\     i_{1_{ops}}     &amp;=\\left(W_{hi}x^T\\right)_{ops}=N\\times H_{out}\\times\\left(2\\times H_{out}-1\\right) \\\\     \\left(i_0+i_1\\right)_{ops} &amp;= N\\times H_{out} \\\\     \\sigma_{ops} &amp;= 3\\times N\\times H_{out}\\\\     i_{ops} &amp;= 2\\times N\\times H_{out}\\times\\left(1+H_{in}+ H_{out}\\right) \\end{align} \\]"},{"location":"modules/lstmcell/#forget-and-output-gates","title":"Forget and output gates","text":"<p>Since the dimensions of these gates are the same as the input gate \\(i\\), it is trivial to observe that</p> \\[ \\begin{equation}     i_{ops}=f_{ops}=o_{ops} \\end{equation} \\]"},{"location":"modules/lstmcell/#cell-gate","title":"Cell gate","text":"<p>The argument of the \\(\\text{tanh}\\) function has the same shape as the previously computed gates, yet the complexity of this function itself is the only difference between this gate and the others, then</p> \\[ \\begin{align}     g_{ops}&amp;=i_{0_{ops}}+i_{1_{ops}}+     \\left(i_0+i_1\\right)_{ops}+\\text{tanh}_{ops}\\\\     \\text{tanh}_{ops}&amp;=7\\times N\\times H_{out}\\\\ \\end{align} \\] <p>Replacing by the previously calculated results</p> \\[ \\begin{equation} g_{ops}=\\begin{cases}     2\\times N\\times H_{out}\\times \\left(4+H_{in}+ H_{out}\\right), &amp; \\text{if}\\ \\text{bias}=\\text{True} \\\\     2\\times N\\times H_{out}\\times \\left(3+H_{in} + H_{out}\\right), &amp;\\text{if}\\ \\text{bias}=\\text{False} \\end{cases} \\end{equation} \\]"},{"location":"modules/lstmcell/#cprime","title":"\\(c\\prime\\)","text":"<p>The complexity of \\(c\\prime\\) corresponds to three element-wise operations between elements with shape \\(\\left(H_{out}, N\\right)\\). Therefore its complexity is</p> \\[ \\begin{align}     c\\prime &amp;= f\\odot c+i\\odot g\\\\     c\\prime_{ops} &amp;= 3\\times N\\times H_{out} \\end{align} \\]"},{"location":"modules/lstmcell/#hprime","title":"\\(h\\prime\\)","text":"<p>The complexity of \\(h\\prime\\) corresponds to one element-wise operation and a \\(\\text{tanh}\\) operation</p> \\[ \\begin{align}     h\\prime &amp;= o \\odot\\text{tanh}\\left(c\\prime\\right)\\\\     \\text{tanh}_{ops}&amp;=7\\times N\\times H_{out}\\\\     h\\prime_{ops} &amp;= 8\\times N\\times H_{out} \\end{align} \\]"},{"location":"modules/lstmcell/#total-complexity","title":"Total complexity","text":"<p>Finally, the total complexity is the sum of all individual contributions</p> \\[ \\begin{align}     \\text{LSTMCell}_{ops}=i_{ops}+f_{ops}+g_{ops}+o_{ops}+c\\prime_{ops}+h\\prime_{ops} \\end{align} \\] <p>In the case of <code>bias=True</code>, the total number of operations is</p> \\[ \\begin{align}     \\text{LSTMCell}_{ops} &amp;= \\underbrace{6 \\times N \\times H_{out}\\times(2+H_{in} + H_{out})}_{i_{ops} + f_{ops} + o_{ops}} \\nonumber \\\\     &amp;\\quad+ \\underbrace{2 \\times N \\times H_{out} \\times (4 + H_{in} + H_{out})}_{g_{ops}} \\nonumber \\\\     &amp;\\quad+ \\underbrace{11 \\times N \\times H_{\\text{out}}}_{c\\prime_{ops}+h\\prime_{ops}} \\\\     \\text{LSTMCell}_{ops} &amp;= 8\\times N\\times H_{out}\\times\\left( H_{in}+H_{out}+3.875\\right) \\end{align} \\] <p>and for <code>bias=False</code></p> \\[ \\begin{align}     \\text{LSTMCell}_{ops} &amp;= \\underbrace{6 \\times N \\times H_{out}\\times(1+H_{in} + H_{out})}_{i_{ops} + f_{ops} + o_{ops}} \\nonumber \\\\     &amp;\\quad+ \\underbrace{2 \\times N \\times H_{out} \\times (3 + H_{in} + H_{out})}_{g_{ops}} \\nonumber \\\\     &amp;\\quad+ \\underbrace{11 \\times N \\times H_{\\text{out}}}_{c\\prime_{ops}+h\\prime_{ops}} \\\\     \\text{LSTMCell}_{ops} &amp;= 8\\times N\\times H_{out}\\times\\left(H_{in}+H_{out}+2.875\\right) \\end{align} \\]"},{"location":"modules/lstmcell/#summary","title":"Summary","text":"<p>The number of operations \\(\\phi\\) operformed by a <code>torch.nn.LSTMCell</code> module can be estimated as</p> If <code>bias=True</code>If <code>bias=False</code> <p>\\(\\text{LSTMCell}_{ops} = 8\\times N\\times H_{out}\\times\\left( H_{in}+H_{out}+3.875\\right)\\)</p> <p>\\(\\text{LSTMCell}_{ops} = 8\\times N\\times H_{out}\\times\\left(H_{in}+H_{out}+2.875\\right)\\)</p> <p>Where</p> <ul> <li>\\(N\\) is the batch size.</li> <li>\\(H_\\text{in}\\) is the number of input features.</li> <li>\\(H_\\text{out}\\) is the number of output features.</li> </ul>"},{"location":"modules/relu/","title":"ReLU (<code>torch.nn.ReLU)</code>","text":"<p>A <code>torch.nn.ReLU</code> corresponds to a Rectified Linear Unit function that can be defined as</p> \\[ \\begin{align}     \\text{ReLU}\\left(x\\right) = \\text{max}\\left(0, x\\right) \\end{align} \\] <p>Where \\(x\\) is the input tensor of any size because \\(\\text{ReLU}\\) is an element-wise activation function.</p>"},{"location":"modules/relu/#complexity","title":"Complexity","text":"<p>For this case, one operation per element is assumed, therefore the total complexity is simply the number of elements of \\(x\\). Given a rank-N tensor \\(x\\) of size \\(\\left(d_0, d_1, \\cdots, d_{N-1}\\right)\\) the number of operations performed by a <code>torch.nn.ReLU</code> module \\(\\text{ReLU}_{ops}\\) is</p> \\[ \\begin{equation} \\text{ReLU}_{ops}=\\prod^{N - 1}d_n=d_0\\times d_1\\times\\cdots\\times d_{N-1} \\end{equation} \\] <p>Note</p> <p>Please note <code>max</code> is not a basic arithmetic operation and the actual number of instructions this function requires may vary. Since <code>moduleprofiler</code> is based on the mathematical relationship between input and output, one operation per element is assumed for this activation function. </p>"},{"location":"modules/relu/#summary","title":"Summary","text":"<p>The number of operations performed by a <code>torch.nn.ReLU</code> module can be estimated as</p> <p>\\(\\text{ReLU}_{ops}=\\displaystyle\\prod^{N - 1}d_n=d_0\\times d_1\\times\\cdots\\times d_{N-1}\\)</p> <p>Where \\(x\\) is a rank-N tensor of size \\(\\left(d_0, d_1, \\cdots, d_{N-1}\\right)\\). </p>"},{"location":"modules/sigmoid/","title":"Sigmoid (<code>torch.nn.Sigmoid</code>)","text":"<p>A <code>torch.nn.Sigmoid</code> corresponds to a sigmoid function that can be defined as</p> \\[ \\begin{align}     \\text{Sigmoid}\\left(x\\right) = \\sigma\\left(x\\right) = \\frac{1}{1+e^{-x}} \\end{align} \\] <p>Where \\(x\\) is the input tensor of any size because \\(\\text{Sigmoid}\\) is an element-wise activation function.</p>"},{"location":"modules/sigmoid/#complexity","title":"Complexity","text":"<p>For this case, there one exponential, one sum and one division. Therefore, the total complexity is simply three times the number of elements in the input tensor \\(x\\). Given a rank-N tensor \\(x\\) of size \\(\\left(d_0, d_1, \\cdots, d_{N-1}\\right)\\) the number of operations performed by a <code>torch.nn.Sigmoid</code> module \\(\\text{Sigmoid}_{ops}\\) is</p> \\[ \\begin{equation}     \\text{Sigmoid}_{ops}=3\\times\\prod^{N - 1}d_n=d_0\\times d_1\\times\\cdots\\times d_{N-1} \\end{equation} \\] <p>Note</p> <p>Please note that calculating an exponential is generally much more complex than performing a single multiplication. However, since the specific implementation details are not covered by this package, we assume an increase of one operation for both cases.</p>"},{"location":"modules/sigmoid/#summary","title":"Summary","text":"<p>The number of operations performed by a <code>torch.nn.Sigmoid</code> module can be estimated as</p> <p>\\(\\text{Sigmoid}_{ops}=3\\times\\prod^{N - 1}d_n=d_0\\times d_1\\times\\cdots\\times d_{N-1}\\)</p> <p>Where \\(x\\) is a rank-N tensor of size \\(\\left(d_0, d_1, \\cdots, d_{N-1}\\right)\\).</p>"},{"location":"modules/softmax/","title":"Softmax (<code>torch.nn.Softmax</code>)","text":"<p>A <code>torch.nn.Softmax</code> applies the softmax function along a given dimension <code>dim</code>. This function is defined as</p> \\[ \\begin{align}     \\text{Softmax}\\left(x_i\\right) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} \\end{align} \\] <p>Where \\(x\\) is the input tensor of any size because \\(\\text{Softmax}\\) is an element-wise activation function.</p> <p>Note</p> <p>This function causes the sum of all values along <code>dim</code> to be <code>1.0</code>. This function is normally used for feature selection and multi-class classification, producing values that can be interpreted as weights for a certain feature, or the probability of a certain outcome. However, it is important to consider in the latter case that such values are not inherently calibrated probabilities, unless the network is explicitly trained for this purpose.</p>"},{"location":"modules/softmax/#complexity","title":"Complexity","text":"<p>The denominator of the computation \\(\\sum_je^{x_j}\\) needs to be calculated only once per row along <code>dim</code>. This calculation involves as many exponential functions as elements along <code>dim</code> and then a sum. If we assume that <code>dim</code> as \\(N\\) elements, then the number of operations is</p> \\[ \\begin{equation} \\left(\\sum_je^{x_j}\\right)_{ops}=2\\times N - 1 \\end{equation} \\] <p>Where \\(N\\) is the number of exponential function operations, and \\(N-1\\) is the number of additions. Then, for the numerator there is a per-element exponential (\\(N\\) exponential operations) and a per element division (\\(N\\) division operations), totalling \\(2 \\times N\\)  additional operations resulting in \\(4\\times N - 1\\) operations per row. This amount of operations will be repeated \\(M\\) times where \\(M\\) corresponds to the dimensions other than dim</p> \\[ \\begin{equation}     M=\\prod_{n\\neq\\text{dim}}^{N - 1}d_n=d_0\\times d_1\\times\\cdots\\times d_{N-1} \\end{equation} \\] <p>Now, the resulting complexity is $$ \\begin{equation}     \\left(\\text{Softmax}\\right)_{ops}=M\\times\\left(4\\times N - 1\\right) \\end{equation} $$</p>"},{"location":"modules/softmax/#summary","title":"Summary","text":"<p>The number of operations performed by a <code>torch.nn.Softmax</code> module can be estimated as</p> <p>\\(\\left(\\text{Softmax}\\right)_{ops}=M\\times\\left(4\\times N - 1\\right)\\)</p> <p>Where</p> <ul> <li>\\(N\\) is the number of elements along dimension <code>dim</code>.</li> <li>\\(M\\) is the product of the size of all other dimensions except <code>dim</code>.</li> </ul>"},{"location":"modules/tanh/","title":"Tanh (<code>torch.nn.Tanh</code>)","text":"<p>A <code>torch.nn.Tanh</code> corresponds to the Hyperbolic Tangent function that can be defined as</p> \\[ \\begin{align}     \\text{Tanh}\\left(x\\right) = \\frac{e^x-e^{-x}}{e^x+e^{-x}} \\end{align} \\] <p>Where \\(x\\) is the input tensor of any size because \\(\\text{Tanh}\\) is an element-wise activation function.</p>"},{"location":"modules/tanh/#complexity","title":"Complexity","text":"<p>For this case, there are four exponentials, one sum, one subtraction and a division. Therefore the total complexity is simply seven times the number of elements in the input tensor \\(x\\). Given a rank-N tensor \\(x\\) of size \\(\\left(d_0, d_1, \\cdots, d_{N-1}\\right)\\) the number of operations performed by a <code>torch.nn.Tanh</code> module \\(\\text{Tanh}_{ops}\\) is</p> \\[ \\begin{equation}     \\text{Tanh}_{ops}=7\\times\\prod^{N - 1}d_n=d_0\\times d_1\\times\\cdots\\times d_{N-1} \\end{equation} \\] <p>Note</p> <p>Please note that calculating an exponential is generally much more complex than performing a single multiplication. However, since the specific implementation details are not covered by this package, we assume an increase of one operation for both cases.</p>"},{"location":"modules/tanh/#summary","title":"Summary","text":"<p>The number of operations performed by a <code>torch.nn.Tanh</code> module can be estimated as</p> <p>\\(\\text{Tanh}_{ops}=7\\times\\prod^{N - 1}d_n=d_0\\times d_1\\times\\cdots\\times d_{N-1}\\)</p> <p>Where \\(x\\) is a rank-N tensor of size \\(\\left(d_0, d_1, \\cdots, d_{N-1}\\right)\\).</p>"}]}